{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f92f8b-817e-478b-8b92-18c576bbdcbd",
   "metadata": {},
   "source": [
    "Text annotation to synthetic image pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8078ca99-b6b7-4217-97c5-23ef0667d784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 13:00:37.784158: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-09 13:00:37.832422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 13:00:38.288411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/saccharinedreams/miniconda3/envs/bitmind/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 13:00:42.535 |       INFO       |  - Loading image generation model (SG161222/RealVisXL_V4.0)... - \n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count, current_process, Manager, get_context\n",
    "\n",
    "# Third-party library imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from transformers import pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.models.modeling_outputs import Transformer2DModelOutput\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "\n",
    "# Local/application-specific imports\n",
    "import bittensor as bt\n",
    "from bitmind.constants import PROMPT_GENERATOR_NAMES, PROMPT_GENERATOR_ARGS, DIFFUSER_NAMES, DIFFUSER_ARGS\n",
    "from multiprocessing_tasks import worker_initializer, generate_images_for_chunk\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1: filter out INFO, 2: additionally filter out WARNING, 3: additionally filter out ERROR)\n",
    "import tensorflow as tf  # Import TensorFlow after setting the log level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8cc9c0-1e2d-4995-af7e-3ff9819d3db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Suppress FutureWarnings from diffusers module\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='diffusers')\n",
    "# Set device for model operations\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if device == \"cpu\":\n",
    "    raise RuntimeError(\"This script requires a GPU because it uses torch.float16.\")  # Added check for GPU availability\n",
    "# Ensure that this script uses 'spawn' method for starting multiprocessing tasks\n",
    "ctx = get_context(\"spawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db788da4-0457-4dfe-9d2e-2493cd68c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datasets(base_dir):\n",
    "    \"\"\"List all subdirectories in the base directory.\"\"\"\n",
    "    return [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "def load_annotations(base_dir, dataset):\n",
    "    \"\"\"Load annotations from JSON files within a specified directory.\"\"\"\n",
    "    annotations = []\n",
    "    path = os.path.join(base_dir, dataset)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(path, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                annotations.append(data)\n",
    "    return annotations\n",
    "\n",
    "def load_diffuser(model_name):\n",
    "    \"\"\"Load a diffusion model by name, configured to provided arguments.\"\"\"\n",
    "    bt.logging.info(f\"Loading image generation model ({model_name})...\")\n",
    "    model = DiffusionPipeline.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float32 if device == \"cpu\" else torch.float16, **DIFFUSER_ARGS[model_name]\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3db054c-21fe-40e0-b7b0-5ec6270a5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU\n",
    "def generate_images(annotations, diffuser, save_dir, num_images, batch_size, diffuser_name):\n",
    "    \"\"\"Generate images from annotations using a diffuser and save to the specified directory.\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    generated_images = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_images, len(annotations))):\n",
    "            start_loop = time.time()\n",
    "            annotation = annotations[i]\n",
    "            prompt = annotation['description']\n",
    "            index = annotation.get('index', f\"missing_index\")\n",
    "\n",
    "            logging.info(f\"Annotation {i}: {json.dumps(annotation, indent=2)}\")\n",
    "\n",
    "            generated_image = diffuser(prompt=prompt).images[0]\n",
    "            logging.info(f\"Type of generated image: {type(generated_image)}\")\n",
    "\n",
    "            if isinstance(generated_image, torch.Tensor):\n",
    "                img = ToPILImage()(generated_image)\n",
    "            else:\n",
    "                img = generated_image\n",
    "\n",
    "            safe_prompt = prompt[:50].replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "            img_filename = f\"{save_dir}/{safe_prompt}-{index}.png\"\n",
    "            img.save(img_filename)\n",
    "            generated_images.append(img_filename)\n",
    "            loop_time = time.time() - start_loop\n",
    "            logging.info(f\"Image saved to {img_filename}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    return generated_images\n",
    "\n",
    "\n",
    "def load_and_initialize_diffuser(diffuser_name, previous_diffuser=None):\n",
    "    \"\"\"Load and initialize the diffuser, handling previous diffuser cleanup if needed.\"\"\"\n",
    "    if previous_diffuser is not None:\n",
    "        logging.info(\"Deleting previous diffuser, freeing memory\")\n",
    "        # Move to float32 if it's float16, then move to CPU for deletion\n",
    "        if previous_diffuser.dtype == torch.float16:\n",
    "            previous_diffuser = previous_diffuser.to(dtype=torch.float32)\n",
    "        previous_diffuser.to('cpu')\n",
    "        del previous_diffuser\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return load_diffuser(diffuser_name)\n",
    "\n",
    "def test_diffuser_on_dataset(dataset, annotations, diffuser, output_dir, num_images, batch_size, diffuser_name):\n",
    "    \"\"\"Test a single diffuser on a given dataset.\"\"\"\n",
    "    dataset_name = dataset.rsplit('/', 1)[-1] if '/' in dataset else dataset\n",
    "    diffuser_name = diffuser_name.rsplit('/', 1)[-1] if '/' in diffuser_name else diffuser_name\n",
    "    save_dir = os.path.join(output_dir, dataset_name, diffuser_name)\n",
    "    logging.info(f\"Testing {diffuser_name} on annotation dataset {dataset} at {save_dir}...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        generate_images(annotations, diffuser, save_dir, num_images, batch_size, diffuser_name)\n",
    "        logging.info(\"Images generated and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate images with {diffuser_name}: {str(e)}\")\n",
    "\n",
    "def cleanup_diffuser(diffuser):\n",
    "    \"\"\"Clean up resources associated with a diffuser.\"\"\"\n",
    "    logging.info(\"Deleting diffuser, freeing memory\")\n",
    "    # Move to float32 if it's float16, then move to CPU for deletion\n",
    "    if diffuser.dtype == torch.float16:\n",
    "        diffuser = diffuser.to(dtype=torch.float32)\n",
    "    diffuser.to('cpu')\n",
    "    del diffuser\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def test_diffusers_on_datasets(annotations_dir, output_dir, num_images=1, batch_size=2):\n",
    "    \"\"\"Test various diffusers on datasets.\"\"\"\n",
    "    datasets = list_datasets(annotations_dir)\n",
    "    for diffuser_name in DIFFUSER_NAMES:\n",
    "        logging.info(f\"Loading and initializing diffuser: {diffuser_name}\")\n",
    "        diffuser = load_and_initialize_diffuser(diffuser_name)\n",
    "        for dataset in datasets:\n",
    "            annotations = load_annotations(annotations_dir, dataset)\n",
    "            test_diffuser_on_dataset(dataset, annotations, diffuser, output_dir, num_images, batch_size, diffuser_name)\n",
    "        cleanup_diffuser(diffuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69b99ce-efbc-413b-822b-3234246643db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiprocessing loop\n",
    "def multiprocess_generate_images(annotations_dir, output_dir, num_processes=None):\n",
    "    if num_processes is None:\n",
    "        num_processes = max(1, cpu_count() - 1)  # Leaves one CPU core free\n",
    "\n",
    "    datasets = list_datasets(annotations_dir)\n",
    "    for model_name in DIFFUSER_NAMES:\n",
    "        logging.info(f\"Processing with model: {model_name}\")\n",
    "        with ctx.Pool(processes=num_processes, initializer=worker_initializer, initargs=(model_name, device, DIFFUSER_ARGS)) as pool:\n",
    "            for dataset in datasets:\n",
    "                annotations = load_annotations(annotations_dir, dataset)\n",
    "                save_dir = os.path.join(output_dir, model_name, dataset)\n",
    "\n",
    "                # Split annotations into chunks for each worker\n",
    "                chunk_size = (len(annotations) + num_processes - 1) // num_processes\n",
    "                chunks = [annotations[i:i + chunk_size] for i in range(0, len(annotations), chunk_size)]\n",
    "\n",
    "                results = pool.starmap(generate_images_for_chunk, [(chunk, save_dir) for chunk in chunks])\n",
    "                logging.info(f\"Completed processing for dataset {dataset} with model {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef374c37-335b-457f-aafd-139f00025d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_DIR = \"test_data/annotations/\"\n",
    "OUTPUT_DIR = \"test_data/synthetics_from_annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ed638-42d4-44be-8d46-fb14c04287fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "test_diffusers_on_datasets(ANNOTATIONS_DIR, OUTPUT_DIR, num_images=1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb4ccf-330f-4982-94cb-c47cd0beb1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CPU Multiprocessing\n",
    "# multiprocess_generate_images(ANNOTATIONS_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90598c4b-4325-47a0-bf92-d207a79cf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_specific_diffuser_on_specific_dataset(annotations_dir, output_dir, diffuser_name=\"SG161222/RealVisXL_V4.0\", dataset_name=\"merkol_ffhq-256\", num_images=1, batch_size=8):\n",
    "    \"\"\"Test a specific diffuser on a specific dataset\"\"\"\n",
    "    logging.info(f\"Loading and initializing diffuser: {diffuser_name}\")\n",
    "    diffuser = load_and_initialize_diffuser(diffuser_name)\n",
    "    annotations = load_annotations(annotations_dir, dataset_name)\n",
    "    \n",
    "    logging.info(f\"Testing diffuser: {diffuser_name} on dataset: {dataset_name}\")\n",
    "    test_diffuser_on_dataset(dataset_name, annotations, diffuser, output_dir, num_images, batch_size, diffuser_name)\n",
    "    \n",
    "    cleanup_diffuser(diffuser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2218a909-dc30-4492-99c5-fbe1c6145e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stabilityai/stable-diffusion-xl-base-1.0',\n",
       " 'SG161222/RealVisXL_V4.0',\n",
       " 'Corcelio/mobius']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFFUSER_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda5c020-9469-4c35-ad95-7a051d031fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading and initializing diffuser: SG161222/RealVisXL_V4.0\n",
      "INFO:bittensor: - Loading image generation model (SG161222/RealVisXL_V4.0)... - \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00aaef0ff941cfa1324213c430bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Testing diffuser: SG161222/RealVisXL_V4.0 on dataset: merkol_ffhq-256\n",
      "INFO:root:Testing RealVisXL_V4.0 on annotation dataset merkol_ffhq-256 at merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0...\n",
      "INFO:root:Annotation 0: {\n",
      "  \"description\": \"A picture of a woman with a white shirt and a necklace.The setting is indoors.The background is a white wall.The woman is smiling..\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266e2a46d0540aea7afb27062e37770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_woman_with_a_white_shirt_and_a_neck-missing_index.png\n",
      "INFO:root:Annotation 1: {\n",
      "  \"description\": \"A picture of a woman wearing a hat and smiling.The setting is outside.The background is a building.The woman is wearing a red, orange and yellow hat.The woman is wearing a.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55867570e14e47fe824a5828f0e92976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_woman_wearing_a_hat_and_smiling.The-missing_index.png\n",
      "INFO:root:Annotation 2: {\n",
      "  \"description\": \"A picture of a girl in a graduation cap and gown smiling.The setting is outdoors.The background is green.The girl is wearing a blue cap and gown..\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfad132450e449659f9304e5b361591e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_girl_in_a_graduation_cap_and_gown_s-missing_index.png\n",
      "INFO:root:Annotation 3: {\n",
      "  \"description\": \"A picture of a young man with glasses looking at his cell phone.The setting is a restaurant.The background is a green wall.The man is wearing a white shirt and black tie..\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6c4f2ce03c408381240f0b7b9d1f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_young_man_with_glasses_looking_at_h-missing_index.png\n",
      "INFO:root:Annotation 4: {\n",
      "  \"description\": \"A picture of a baby girl with a pacifier in her mouth.The setting is indoors.The background is a wall.The baby is wearing a pink sweater.The baby is holding a stuffed animal.The.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5044c231bf2a46029381b879783e2c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_baby_girl_with_a_pacifier_in_her_mo-missing_index.png\n",
      "INFO:root:Annotation 5: {\n",
      "  \"description\": \"A picture of a young boy holding a stuffed animal.The setting is a library.The background is a book shelf.The boy is smiling.The boy is holding a stuffed animal.The boy is.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97d208c284f4fa380bc0db2ab20fde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_young_boy_holding_a_stuffed_animal.-missing_index.png\n",
      "INFO:root:Annotation 6: {\n",
      "  \"description\": \"A picture of a woman talking on a cell phone.The setting is outdoors.The background is blurred.The woman is wearing a black jacket.The woman is looking at something.The woman is.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057b859cb0b94b1a97061b528adba68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_woman_talking_on_a_cell_phone.The_s-missing_index.png\n",
      "INFO:root:Annotation 7: {\n",
      "  \"description\": \"A picture of a man wearing a hat and glasses smiling.The setting is outdoors.The background is a tree.The man is wearing a blue hat.The man is smiling.The man is wearing.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6073c9c17eb44de864671e230b93e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_man_wearing_a_hat_and_glasses_smili-missing_index.png\n",
      "INFO:root:Annotation 8: {\n",
      "  \"description\": \"A picture of a baby with a fake mustache and fake mustache.The setting is a white wall.The background is a blue wall.The baby is wearing a blue shirt.The baby is smiling.The baby is.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8224885635064028b570ac20058bff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_baby_with_a_fake_mustache_and_fake_-missing_index.png\n",
      "INFO:root:Annotation 9: {\n",
      "  \"description\": \"A picture of a woman with curly hair holding a cell phone.The setting is indoors.The background is a wall.The woman is smiling.The woman is wearing a white shirt.The woman is holding.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806b73dd74ce49978da82c19b94f2990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Type of generated image: <class 'PIL.Image.Image'>\n",
      "INFO:root:Image saved to merkol_realvis_test/merkol_ffhq-256/RealVisXL_V4.0/A_picture_of_a_woman_with_curly_hair_holding_a_cel-missing_index.png\n",
      "INFO:root:Total processing time: 1382.97 seconds\n",
      "INFO:root:Images generated and saved successfully.\n",
      "INFO:root:Deleting diffuser, freeing memory\n"
     ]
    }
   ],
   "source": [
    "merkol = \"merkol_realvis_test/\"\n",
    "test_specific_diffuser_on_specific_dataset(ANNOTATIONS_DIR, merkol, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56669618-bf59-4838-8e07-ae47193ac8a2",
   "metadata": {},
   "source": [
    "Image Comparison Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65bc01-64aa-4a67-a61e-a6225f53fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "# Bitmind-specific libraries\n",
    "from bitmind.image_dataset import ImageDataset\n",
    "from bitmind.constants import DATASET_META\n",
    "\n",
    "# Initialize seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Suppress logs\n",
    "transformers_level = logging.getLogger(\"transformers\").getEffectiveLevel()\n",
    "huggingface_hub_level = logging.getLogger(\"huggingface_hub\").getEffectiveLevel()\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b-coco\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b-coco\", torch_dtype=torch.float16) \n",
    "model.to(device)\n",
    "\n",
    "# Restore log settings\n",
    "logging.getLogger(\"transformers\").setLevel(transformers_level)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(huggingface_hub_level)\n",
    "\n",
    "# Load real datasets\n",
    "print(\"Loading real datasets\")\n",
    "real_image_datasets = [\n",
    "    ImageDataset(ds['path'], 'test', ds.get('name', None), ds['create_splits'])\n",
    "    for ds in DATASET_META['real']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12e3f8-319a-4388-8132-9dfa34d75c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset name and index of image to display\n",
    "target_dataset_name = 'dalle-mini/open-images'\n",
    "index_of_image = 10\n",
    "\n",
    "for dataset in real_image_datasets:\n",
    "    if dataset.huggingface_dataset_path == target_dataset_name:\n",
    "        for index, image_info in enumerate(dataset):\n",
    "            if index == index_of_image:\n",
    "                display(image_info['image'])\n",
    "                break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeda3af-5865-4ea2-b396-b5365aafa956",
   "metadata": {},
   "source": [
    "#### To-do\n",
    "-Test image sizes\n",
    "-Set up evaluation for real image counterpart and synthetic generated from annotation of said real image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
