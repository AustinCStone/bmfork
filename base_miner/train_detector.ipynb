{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5214826-521c-43ba-ae33-cc2b4e4296fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gustavosta/MagicPrompt-Stable-Diffusion': {'model': 'Gustavosta/MagicPrompt-Stable-Diffusion', 'tokenizer': 'gpt2', 'device': -1}}\n",
      "{'stabilityai/stable-diffusion-xl-base-1.0': {'use_safetensors': True, 'variant': 'fp16'}, 'SG161222/RealVisXL_V4.0': {'use_safetensors': True, 'variant': 'fp16'}, 'Corcelio/mobius': {'use_safetensors': True}}\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from validate import validate\n",
    "from networks.trainer import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "\n",
    "#from base_miner.util import Logger\n",
    "from util.data import load_datasets, create_real_fake_datasets\n",
    "from options import TrainOptions\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d887887d-d8bd-4034-a394-9bec29b99428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "                     arch: res50                         \n",
      "               batch_size: 64                            \n",
      "                    beta1: 0.9                           \n",
      "                blur_prob: 0                             \n",
      "                 blur_sig: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                class_bal: False                         \n",
      "                  classes:                               \n",
      "           continue_train: False                         \n",
      "                 cropSize: 224                           \n",
      "                 data_aug: False                         \n",
      "                 dataroot: ./dataset/                    \n",
      "                delr_freq: 20                            \n",
      "          earlystop_epoch: 15                            \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                  isTrain: True                          \t[default: None]\n",
      "               jpg_method: cv2                           \n",
      "                 jpg_prob: 0                             \n",
      "                 jpg_qual: 75                            \n",
      "               last_epoch: -1                            \n",
      "                 loadSize: 256                           \n",
      "                loss_freq: 400                           \n",
      "                       lr: 0.0001                        \n",
      "                     mode: binary                        \n",
      "                     name: experiment_name2024_06_11_05_55_10\t[default: experiment_name]\n",
      "                new_optim: False                         \n",
      "                    niter: 1000                          \n",
      "                  no_flip: False                         \n",
      "              num_threads: 8                             \n",
      "                    optim: adam                          \n",
      "           resize_or_crop: scale_and_crop                \n",
      "                rz_interp: bilinear                      \n",
      "          save_epoch_freq: 20                            \n",
      "         save_latest_freq: 2000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "              train_split: train                         \n",
      "                val_split: val                           \n",
      "----------------- End -------------------\n",
      "[{'path': 'bitmind/RealVisXL_V4.0_images', 'create_splits': True}, {'path': 'poloclub/diffusiondb', 'name': 'large_first_10k', 'create_splits': True}]\n",
      "Loading bitmind/RealVisXL_V4.0_images[train] ... done, len=2352\n",
      "Loading poloclub/diffusiondb[train] ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/bmsn/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for poloclub/diffusiondb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/poloclub/diffusiondb\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, len=8000\n",
      "Loading bitmind/RealVisXL_V4.0_images[validation] ... done, len=294\n",
      "Loading poloclub/diffusiondb[validation] ... done, len=1000\n",
      "Loading bitmind/RealVisXL_V4.0_images[test] ... done, len=294\n",
      "Loading poloclub/diffusiondb[test] ... done, len=1000\n",
      "Loading dalle-mini/open-images[train] ... done, len=9011219\n",
      "Loading merkol/ffhq-256[train] ... done, len=56000\n",
      "Loading jlbaker361/flickr_humans_20k[train] ... done, len=16000\n",
      "Loading saitsharipov/CelebA-HQ[train] ... done, len=162079\n",
      "Loading dalle-mini/open-images[validation] ... done, len=41620\n",
      "Loading merkol/ffhq-256[validation] ... done, len=7000\n",
      "Loading jlbaker361/flickr_humans_20k[validation] ... done, len=2000\n",
      "Loading saitsharipov/CelebA-HQ[validation] ... done, len=20260\n",
      "Loading dalle-mini/open-images[test] ... done, len=125436\n",
      "Loading merkol/ffhq-256[test] ... done, len=7000\n",
      "Loading jlbaker361/flickr_humans_20k[test] ... done, len=2000\n",
      "Loading saitsharipov/CelebA-HQ[test] ... done, len=20260\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse()\n",
    "seed_torch(100)\n",
    "\n",
    "#Logger(os.path.join(opt.checkpoints_dir, opt.name, 'log.log'))\n",
    "\n",
    "train_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, \"train\"))\n",
    "val_writer = SummaryWriter(os.path.join(opt.checkpoints_dir, opt.name, \"val\"))\n",
    "\n",
    "real_datasets, fake_datasets = load_datasets()\n",
    "train_dataset, val_dataset, test_dataset = create_real_fake_datasets(real_datasets, fake_datasets)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=lambda d: tuple(d))\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=lambda d: tuple(d))\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=lambda d: tuple(d))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d24a8e-3217-48b2-a233-1b29d0d6c3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2352, 294, 294)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112a889f-bef2-45b5-99dc-2fdde8b51852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10344827586206896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 / 2900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184fda9e-0397-4037-89bf-bfc5177af3e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/user/bitmind-subnet/base_miner\n",
      "2024_06_11_05_57_13 Train loss: 0.6451554298400879 at step: 1 lr 0.0001\n",
      "2024_06_11_05_57_14 Train loss: 0.652596652507782 at step: 2 lr 0.0001\n",
      "2024_06_11_05_57_16 Train loss: 0.7342991828918457 at step: 3 lr 0.0001\n",
      "2024_06_11_05_57_18 Train loss: 0.6682878732681274 at step: 4 lr 0.0001\n",
      "2024_06_11_05_57_19 Train loss: 0.6569811105728149 at step: 5 lr 0.0001\n",
      "2024_06_11_05_57_20 Train loss: 0.6456738114356995 at step: 6 lr 0.0001\n",
      "2024_06_11_05_57_21 Train loss: 0.7409511804580688 at step: 7 lr 0.0001\n",
      "2024_06_11_05_57_23 Train loss: 0.7689532041549683 at step: 8 lr 0.0001\n",
      "2024_06_11_05_57_24 Train loss: 0.6740535497665405 at step: 9 lr 0.0001\n",
      "2024_06_11_05_57_26 Train loss: 0.7736349105834961 at step: 10 lr 0.0001\n",
      "2024_06_11_05_57_27 Train loss: 0.7192068099975586 at step: 11 lr 0.0001\n",
      "2024_06_11_05_57_28 Train loss: 0.6234734058380127 at step: 12 lr 0.0001\n",
      "2024_06_11_05_57_29 Train loss: 0.7114592790603638 at step: 13 lr 0.0001\n",
      "2024_06_11_05_57_30 Train loss: 0.6468729972839355 at step: 14 lr 0.0001\n",
      "2024_06_11_05_57_32 Train loss: 0.6714909076690674 at step: 15 lr 0.0001\n",
      "2024_06_11_05_57_33 Train loss: 0.628865659236908 at step: 16 lr 0.0001\n",
      "2024_06_11_05_57_34 Train loss: 0.6073372960090637 at step: 17 lr 0.0001\n",
      "2024_06_11_05_57_35 Train loss: 0.6530252695083618 at step: 18 lr 0.0001\n",
      "2024_06_11_05_57_36 Train loss: 0.5562500357627869 at step: 19 lr 0.0001\n",
      "2024_06_11_05_57_37 Train loss: 0.6146148443222046 at step: 20 lr 0.0001\n",
      "2024_06_11_05_57_39 Train loss: 0.5900751948356628 at step: 21 lr 0.0001\n",
      "2024_06_11_05_57_40 Train loss: 0.5548118948936462 at step: 22 lr 0.0001\n",
      "2024_06_11_05_57_42 Train loss: 0.6228384375572205 at step: 23 lr 0.0001\n",
      "2024_06_11_05_57_43 Train loss: 0.5273955464363098 at step: 24 lr 0.0001\n",
      "2024_06_11_05_57_44 Train loss: 0.676862359046936 at step: 25 lr 0.0001\n",
      "2024_06_11_05_57_45 Train loss: 0.4947642982006073 at step: 26 lr 0.0001\n",
      "2024_06_11_05_57_46 Train loss: 0.6568760275840759 at step: 27 lr 0.0001\n",
      "2024_06_11_05_57_48 Train loss: 0.6210573315620422 at step: 28 lr 0.0001\n",
      "2024_06_11_05_57_49 Train loss: 0.5643550753593445 at step: 29 lr 0.0001\n",
      "2024_06_11_05_57_50 Train loss: 0.6523622870445251 at step: 30 lr 0.0001\n",
      "2024_06_11_05_57_51 Train loss: 0.570963978767395 at step: 31 lr 0.0001\n",
      "2024_06_11_05_57_53 Train loss: 0.6660698652267456 at step: 32 lr 0.0001\n",
      "2024_06_11_05_57_54 Train loss: 0.710587203502655 at step: 33 lr 0.0001\n",
      "2024_06_11_05_57_56 Train loss: 0.8061890602111816 at step: 34 lr 0.0001\n",
      "2024_06_11_05_57_57 Train loss: 0.5178706049919128 at step: 35 lr 0.0001\n",
      "2024_06_11_05_57_58 Train loss: 0.6496042609214783 at step: 36 lr 0.0001\n",
      "2024_06_11_05_58_00 Train loss: 0.6074303388595581 at step: 37 lr 0.0001\n",
      "2024_06_11_05_58_01 Train loss: 0.6652126312255859 at step: 38 lr 0.0001\n",
      "2024_06_11_05_58_01 Train loss: 0.6351695656776428 at step: 39 lr 0.0001\n",
      "2024_06_11_05_58_02 Train loss: 0.6590077877044678 at step: 40 lr 0.0001\n",
      "2024_06_11_05_58_03 Train loss: 0.5673873424530029 at step: 41 lr 0.0001\n",
      "2024_06_11_05_58_04 Train loss: 0.6446059346199036 at step: 42 lr 0.0001\n",
      "2024_06_11_05_58_06 Train loss: 0.6866973042488098 at step: 43 lr 0.0001\n",
      "2024_06_11_05_58_07 Train loss: 0.563904881477356 at step: 44 lr 0.0001\n",
      "2024_06_11_05_58_08 Train loss: 0.558761477470398 at step: 45 lr 0.0001\n",
      "2024_06_11_05_58_09 Train loss: 0.5648645162582397 at step: 46 lr 0.0001\n",
      "2024_06_11_05_58_10 Train loss: 0.6420189142227173 at step: 47 lr 0.0001\n",
      "2024_06_11_05_58_11 Train loss: 0.5071636438369751 at step: 48 lr 0.0001\n",
      "2024_06_11_05_58_13 Train loss: 0.6150344610214233 at step: 49 lr 0.0001\n",
      "2024_06_11_05_58_14 Train loss: 0.570852518081665 at step: 50 lr 0.0001\n",
      "2024_06_11_05_58_15 Train loss: 0.5279148817062378 at step: 51 lr 0.0001\n",
      "2024_06_11_05_58_16 Train loss: 0.5983698964118958 at step: 52 lr 0.0001\n",
      "2024_06_11_05_58_17 Train loss: 0.5648525953292847 at step: 53 lr 0.0001\n",
      "2024_06_11_05_58_18 Train loss: 0.6027692556381226 at step: 54 lr 0.0001\n",
      "2024_06_11_05_58_20 Train loss: 0.5153346061706543 at step: 55 lr 0.0001\n",
      "2024_06_11_05_58_23 Train loss: 0.6341805458068848 at step: 56 lr 0.0001\n",
      "2024_06_11_05_58_25 Train loss: 0.6771146655082703 at step: 57 lr 0.0001\n",
      "2024_06_11_05_58_30 Train loss: 0.6558148860931396 at step: 58 lr 0.0001\n",
      "2024_06_11_05_58_32 Train loss: 0.5992958545684814 at step: 59 lr 0.0001\n",
      "2024_06_11_05_58_34 Train loss: 0.5946754217147827 at step: 60 lr 0.0001\n",
      "2024_06_11_05_58_35 Train loss: 0.5243247747421265 at step: 61 lr 0.0001\n",
      "2024_06_11_05_58_36 Train loss: 0.42360296845436096 at step: 62 lr 0.0001\n",
      "2024_06_11_05_58_37 Train loss: 0.40484750270843506 at step: 63 lr 0.0001\n",
      "2024_06_11_05_58_38 Train loss: 0.48312970995903015 at step: 64 lr 0.0001\n",
      "2024_06_11_05_58_39 Train loss: 0.5669659972190857 at step: 65 lr 0.0001\n",
      "2024_06_11_05_58_40 Train loss: 0.5251184701919556 at step: 66 lr 0.0001\n",
      "2024_06_11_05_58_42 Train loss: 0.5266751050949097 at step: 67 lr 0.0001\n",
      "2024_06_11_05_58_43 Train loss: 0.5801334977149963 at step: 68 lr 0.0001\n",
      "2024_06_11_05_58_44 Train loss: 0.44503986835479736 at step: 69 lr 0.0001\n",
      "2024_06_11_05_58_45 Train loss: 0.36686286330223083 at step: 70 lr 0.0001\n",
      "2024_06_11_05_58_46 Train loss: 0.5237094163894653 at step: 71 lr 0.0001\n",
      "2024_06_11_05_58_48 Train loss: 0.5215327739715576 at step: 72 lr 0.0001\n",
      "2024_06_11_05_58_49 Train loss: 0.4362962543964386 at step: 73 lr 0.0001\n",
      "2024_06_11_05_58_50 Train loss: 0.457640677690506 at step: 74 lr 0.0001\n",
      "(Val @ epoch 0) acc: 0.5748299319727891; ap: 0.5933972554444232\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_05_59_04 Train loss: 0.487307608127594 at step: 75 lr 0.0001\n",
      "2024_06_11_05_59_05 Train loss: 0.35682547092437744 at step: 76 lr 0.0001\n",
      "2024_06_11_05_59_06 Train loss: 0.5122944116592407 at step: 77 lr 0.0001\n",
      "2024_06_11_05_59_07 Train loss: 0.417026162147522 at step: 78 lr 0.0001\n",
      "2024_06_11_05_59_09 Train loss: 0.4223891496658325 at step: 79 lr 0.0001\n",
      "2024_06_11_05_59_10 Train loss: 0.3297514319419861 at step: 80 lr 0.0001\n",
      "2024_06_11_05_59_11 Train loss: 0.3044864237308502 at step: 81 lr 0.0001\n",
      "2024_06_11_05_59_13 Train loss: 0.33533644676208496 at step: 82 lr 0.0001\n",
      "2024_06_11_05_59_15 Train loss: 0.500826895236969 at step: 83 lr 0.0001\n",
      "2024_06_11_05_59_17 Train loss: 0.4997525215148926 at step: 84 lr 0.0001\n",
      "2024_06_11_05_59_18 Train loss: 0.43757015466690063 at step: 85 lr 0.0001\n",
      "2024_06_11_05_59_19 Train loss: 0.375482439994812 at step: 86 lr 0.0001\n",
      "2024_06_11_05_59_21 Train loss: 0.5824002027511597 at step: 87 lr 0.0001\n",
      "2024_06_11_05_59_23 Train loss: 0.6588093638420105 at step: 88 lr 0.0001\n",
      "2024_06_11_05_59_23 Train loss: 0.4609454274177551 at step: 89 lr 0.0001\n",
      "2024_06_11_05_59_24 Train loss: 0.4763117730617523 at step: 90 lr 0.0001\n",
      "2024_06_11_05_59_25 Train loss: 0.5801126956939697 at step: 91 lr 0.0001\n",
      "2024_06_11_05_59_27 Train loss: 0.5695434808731079 at step: 92 lr 0.0001\n",
      "2024_06_11_05_59_28 Train loss: 0.5843622088432312 at step: 93 lr 0.0001\n",
      "2024_06_11_05_59_30 Train loss: 0.3446916937828064 at step: 94 lr 0.0001\n",
      "2024_06_11_05_59_31 Train loss: 0.3576081395149231 at step: 95 lr 0.0001\n",
      "2024_06_11_05_59_31 Train loss: 0.34118330478668213 at step: 96 lr 0.0001\n",
      "2024_06_11_05_59_33 Train loss: 0.42349785566329956 at step: 97 lr 0.0001\n",
      "2024_06_11_05_59_34 Train loss: 0.4362117052078247 at step: 98 lr 0.0001\n",
      "2024_06_11_05_59_36 Train loss: 0.571828305721283 at step: 99 lr 0.0001\n",
      "2024_06_11_05_59_36 Train loss: 0.198834627866745 at step: 100 lr 0.0001\n",
      "2024_06_11_05_59_37 Train loss: 0.28988033533096313 at step: 101 lr 0.0001\n",
      "2024_06_11_05_59_38 Train loss: 0.2678476572036743 at step: 102 lr 0.0001\n",
      "2024_06_11_05_59_39 Train loss: 0.3050181269645691 at step: 103 lr 0.0001\n",
      "2024_06_11_05_59_41 Train loss: 0.47899994254112244 at step: 104 lr 0.0001\n",
      "2024_06_11_05_59_42 Train loss: 0.31976860761642456 at step: 105 lr 0.0001\n",
      "2024_06_11_05_59_43 Train loss: 0.3031356930732727 at step: 106 lr 0.0001\n",
      "2024_06_11_05_59_45 Train loss: 0.3090171217918396 at step: 107 lr 0.0001\n",
      "2024_06_11_05_59_47 Train loss: 0.5457407236099243 at step: 108 lr 0.0001\n",
      "2024_06_11_05_59_49 Train loss: 0.31184154748916626 at step: 109 lr 0.0001\n",
      "2024_06_11_05_59_51 Train loss: 0.6082258820533752 at step: 110 lr 0.0001\n",
      "2024_06_11_05_59_52 Train loss: 0.3235055208206177 at step: 111 lr 0.0001\n",
      "2024_06_11_05_59_53 Train loss: 0.43885689973831177 at step: 112 lr 0.0001\n",
      "2024_06_11_05_59_54 Train loss: 0.4925541579723358 at step: 113 lr 0.0001\n",
      "2024_06_11_05_59_55 Train loss: 0.4120163023471832 at step: 114 lr 0.0001\n",
      "2024_06_11_05_59_56 Train loss: 0.34027838706970215 at step: 115 lr 0.0001\n",
      "2024_06_11_05_59_57 Train loss: 0.3427060544490814 at step: 116 lr 0.0001\n",
      "2024_06_11_05_59_59 Train loss: 0.513637363910675 at step: 117 lr 0.0001\n",
      "2024_06_11_06_00_00 Train loss: 0.2602318823337555 at step: 118 lr 0.0001\n",
      "2024_06_11_06_00_00 Train loss: 0.30518415570259094 at step: 119 lr 0.0001\n",
      "2024_06_11_06_00_01 Train loss: 0.3652147054672241 at step: 120 lr 0.0001\n",
      "2024_06_11_06_00_02 Train loss: 0.3684242069721222 at step: 121 lr 0.0001\n",
      "2024_06_11_06_00_04 Train loss: 0.34888410568237305 at step: 122 lr 0.0001\n",
      "2024_06_11_06_00_05 Train loss: 0.4023321270942688 at step: 123 lr 0.0001\n",
      "2024_06_11_06_00_06 Train loss: 0.3133391737937927 at step: 124 lr 0.0001\n",
      "2024_06_11_06_00_07 Train loss: 0.3015950322151184 at step: 125 lr 0.0001\n",
      "2024_06_11_06_00_08 Train loss: 0.30238717794418335 at step: 126 lr 0.0001\n",
      "2024_06_11_06_00_09 Train loss: 0.3640584945678711 at step: 127 lr 0.0001\n",
      "2024_06_11_06_00_10 Train loss: 0.4194338917732239 at step: 128 lr 0.0001\n",
      "2024_06_11_06_00_11 Train loss: 0.34802451729774475 at step: 129 lr 0.0001\n",
      "2024_06_11_06_00_12 Train loss: 0.5144648551940918 at step: 130 lr 0.0001\n",
      "2024_06_11_06_00_14 Train loss: 0.40633898973464966 at step: 131 lr 0.0001\n",
      "2024_06_11_06_00_15 Train loss: 0.3725830912590027 at step: 132 lr 0.0001\n",
      "2024_06_11_06_00_17 Train loss: 0.433907687664032 at step: 133 lr 0.0001\n",
      "2024_06_11_06_00_19 Train loss: 0.4854222536087036 at step: 134 lr 0.0001\n",
      "2024_06_11_06_00_20 Train loss: 0.3321557343006134 at step: 135 lr 0.0001\n",
      "2024_06_11_06_00_21 Train loss: 0.38713306188583374 at step: 136 lr 0.0001\n",
      "2024_06_11_06_00_22 Train loss: 0.458041250705719 at step: 137 lr 0.0001\n",
      "2024_06_11_06_00_24 Train loss: 0.35675257444381714 at step: 138 lr 0.0001\n",
      "2024_06_11_06_00_25 Train loss: 0.45403212308883667 at step: 139 lr 0.0001\n",
      "2024_06_11_06_00_26 Train loss: 0.28489744663238525 at step: 140 lr 0.0001\n",
      "2024_06_11_06_00_27 Train loss: 0.2878785729408264 at step: 141 lr 0.0001\n",
      "2024_06_11_06_00_29 Train loss: 0.5453974604606628 at step: 142 lr 0.0001\n",
      "2024_06_11_06_00_31 Train loss: 0.7058162689208984 at step: 143 lr 0.0001\n",
      "2024_06_11_06_00_32 Train loss: 0.2784852683544159 at step: 144 lr 0.0001\n",
      "2024_06_11_06_00_33 Train loss: 0.29605939984321594 at step: 145 lr 0.0001\n",
      "2024_06_11_06_00_34 Train loss: 0.47457563877105713 at step: 146 lr 0.0001\n",
      "2024_06_11_06_00_35 Train loss: 0.47753459215164185 at step: 147 lr 0.0001\n",
      "2024_06_11_06_00_36 Train loss: 0.4175775349140167 at step: 148 lr 0.0001\n",
      "(Val @ epoch 1) acc: 0.8741496598639455; ap: 0.9303310769996481\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_06_00_53 Train loss: 0.18504680693149567 at step: 149 lr 0.0001\n",
      "2024_06_11_06_00_54 Train loss: 0.3372494578361511 at step: 150 lr 0.0001\n",
      "2024_06_11_06_00_55 Train loss: 0.4951958656311035 at step: 151 lr 0.0001\n",
      "2024_06_11_06_00_56 Train loss: 0.2585923671722412 at step: 152 lr 0.0001\n",
      "2024_06_11_06_00_57 Train loss: 0.36367902159690857 at step: 153 lr 0.0001\n",
      "2024_06_11_06_00_59 Train loss: 0.33805736899375916 at step: 154 lr 0.0001\n",
      "2024_06_11_06_01_00 Train loss: 0.2975342273712158 at step: 155 lr 0.0001\n",
      "2024_06_11_06_01_02 Train loss: 0.47666049003601074 at step: 156 lr 0.0001\n",
      "2024_06_11_06_01_03 Train loss: 0.3058924376964569 at step: 157 lr 0.0001\n",
      "2024_06_11_06_01_05 Train loss: 0.28283846378326416 at step: 158 lr 0.0001\n",
      "2024_06_11_06_01_06 Train loss: 0.40310579538345337 at step: 159 lr 0.0001\n",
      "2024_06_11_06_01_08 Train loss: 0.27464812994003296 at step: 160 lr 0.0001\n",
      "2024_06_11_06_01_09 Train loss: 0.3500969409942627 at step: 161 lr 0.0001\n",
      "2024_06_11_06_01_13 Train loss: 0.31214648485183716 at step: 162 lr 0.0001\n",
      "2024_06_11_06_01_15 Train loss: 0.3188801407814026 at step: 163 lr 0.0001\n",
      "2024_06_11_06_01_16 Train loss: 0.38511916995048523 at step: 164 lr 0.0001\n",
      "2024_06_11_06_01_17 Train loss: 0.26972439885139465 at step: 165 lr 0.0001\n",
      "2024_06_11_06_01_19 Train loss: 0.40891599655151367 at step: 166 lr 0.0001\n",
      "2024_06_11_06_01_21 Train loss: 0.5023678541183472 at step: 167 lr 0.0001\n",
      "2024_06_11_06_01_22 Train loss: 0.18039819598197937 at step: 168 lr 0.0001\n",
      "2024_06_11_06_01_23 Train loss: 0.406483918428421 at step: 169 lr 0.0001\n",
      "2024_06_11_06_01_24 Train loss: 0.42288392782211304 at step: 170 lr 0.0001\n",
      "2024_06_11_06_01_25 Train loss: 0.2965090870857239 at step: 171 lr 0.0001\n",
      "2024_06_11_06_01_26 Train loss: 0.34876319766044617 at step: 172 lr 0.0001\n",
      "2024_06_11_06_01_28 Train loss: 0.39315542578697205 at step: 173 lr 0.0001\n",
      "2024_06_11_06_01_29 Train loss: 0.3401075601577759 at step: 174 lr 0.0001\n",
      "2024_06_11_06_01_30 Train loss: 0.32384368777275085 at step: 175 lr 0.0001\n",
      "2024_06_11_06_01_32 Train loss: 0.36402443051338196 at step: 176 lr 0.0001\n",
      "2024_06_11_06_01_33 Train loss: 0.16738708317279816 at step: 177 lr 0.0001\n",
      "2024_06_11_06_01_35 Train loss: 0.4022808372974396 at step: 178 lr 0.0001\n",
      "2024_06_11_06_01_38 Train loss: 0.6142925024032593 at step: 179 lr 0.0001\n",
      "2024_06_11_06_01_41 Train loss: 0.5739201903343201 at step: 180 lr 0.0001\n",
      "2024_06_11_06_01_42 Train loss: 0.32422935962677 at step: 181 lr 0.0001\n",
      "2024_06_11_06_01_42 Train loss: 0.38697463274002075 at step: 182 lr 0.0001\n",
      "2024_06_11_06_01_44 Train loss: 0.4068862199783325 at step: 183 lr 0.0001\n",
      "2024_06_11_06_01_45 Train loss: 0.28228867053985596 at step: 184 lr 0.0001\n",
      "2024_06_11_06_01_46 Train loss: 0.23017776012420654 at step: 185 lr 0.0001\n",
      "2024_06_11_06_01_48 Train loss: 0.5837728977203369 at step: 186 lr 0.0001\n",
      "2024_06_11_06_01_49 Train loss: 0.6418449878692627 at step: 187 lr 0.0001\n",
      "2024_06_11_06_01_51 Train loss: 0.40433961153030396 at step: 188 lr 0.0001\n",
      "2024_06_11_06_01_51 Train loss: 0.3069801926612854 at step: 189 lr 0.0001\n",
      "2024_06_11_06_01_52 Train loss: 0.31705325841903687 at step: 190 lr 0.0001\n",
      "2024_06_11_06_01_54 Train loss: 0.3030032515525818 at step: 191 lr 0.0001\n",
      "2024_06_11_06_01_55 Train loss: 0.276117205619812 at step: 192 lr 0.0001\n",
      "2024_06_11_06_01_56 Train loss: 0.25048360228538513 at step: 193 lr 0.0001\n",
      "2024_06_11_06_01_57 Train loss: 0.5748385787010193 at step: 194 lr 0.0001\n",
      "2024_06_11_06_01_59 Train loss: 0.6120446920394897 at step: 195 lr 0.0001\n",
      "2024_06_11_06_02_00 Train loss: 0.5017274618148804 at step: 196 lr 0.0001\n",
      "2024_06_11_06_02_01 Train loss: 0.3163403868675232 at step: 197 lr 0.0001\n",
      "2024_06_11_06_02_02 Train loss: 0.30094295740127563 at step: 198 lr 0.0001\n",
      "2024_06_11_06_02_03 Train loss: 0.3951631188392639 at step: 199 lr 0.0001\n",
      "2024_06_11_06_02_04 Train loss: 0.26607218384742737 at step: 200 lr 0.0001\n",
      "2024_06_11_06_02_06 Train loss: 0.5640137195587158 at step: 201 lr 0.0001\n",
      "2024_06_11_06_02_07 Train loss: 0.2876925468444824 at step: 202 lr 0.0001\n",
      "2024_06_11_06_02_08 Train loss: 0.5168869495391846 at step: 203 lr 0.0001\n",
      "2024_06_11_06_02_09 Train loss: 0.21621504426002502 at step: 204 lr 0.0001\n",
      "2024_06_11_06_02_10 Train loss: 0.48624590039253235 at step: 205 lr 0.0001\n",
      "2024_06_11_06_02_11 Train loss: 0.3124527335166931 at step: 206 lr 0.0001\n",
      "2024_06_11_06_02_12 Train loss: 0.35548216104507446 at step: 207 lr 0.0001\n",
      "2024_06_11_06_02_13 Train loss: 0.3393867611885071 at step: 208 lr 0.0001\n",
      "2024_06_11_06_02_14 Train loss: 0.3565947711467743 at step: 209 lr 0.0001\n",
      "2024_06_11_06_02_15 Train loss: 0.37948548793792725 at step: 210 lr 0.0001\n",
      "2024_06_11_06_02_16 Train loss: 0.3377207815647125 at step: 211 lr 0.0001\n",
      "2024_06_11_06_02_18 Train loss: 0.38526439666748047 at step: 212 lr 0.0001\n",
      "2024_06_11_06_02_20 Train loss: 0.2608429193496704 at step: 213 lr 0.0001\n",
      "2024_06_11_06_02_20 Train loss: 0.3446981608867645 at step: 214 lr 0.0001\n",
      "2024_06_11_06_02_21 Train loss: 0.3560715913772583 at step: 215 lr 0.0001\n",
      "2024_06_11_06_02_22 Train loss: 0.18629124760627747 at step: 216 lr 0.0001\n",
      "2024_06_11_06_02_24 Train loss: 0.4487348198890686 at step: 217 lr 0.0001\n",
      "2024_06_11_06_02_26 Train loss: 0.23353803157806396 at step: 218 lr 0.0001\n",
      "2024_06_11_06_02_28 Train loss: 0.36983731389045715 at step: 219 lr 0.0001\n",
      "2024_06_11_06_02_30 Train loss: 0.3651358485221863 at step: 220 lr 0.0001\n",
      "2024_06_11_06_02_31 Train loss: 0.3619610667228699 at step: 221 lr 0.0001\n",
      "2024_06_11_06_02_33 Train loss: 0.6724221110343933 at step: 222 lr 0.0001\n",
      "(Val @ epoch 2) acc: 0.8231292517006803; ap: 0.8723316377009237\n",
      "2024_06_11_06_02_47 Train loss: 0.2871885895729065 at step: 223 lr 0.0001\n",
      "2024_06_11_06_02_49 Train loss: 0.23872604966163635 at step: 224 lr 0.0001\n",
      "2024_06_11_06_02_50 Train loss: 0.3373563885688782 at step: 225 lr 0.0001\n",
      "2024_06_11_06_02_51 Train loss: 0.44486087560653687 at step: 226 lr 0.0001\n",
      "2024_06_11_06_02_53 Train loss: 0.43335241079330444 at step: 227 lr 0.0001\n",
      "2024_06_11_06_02_54 Train loss: 0.3325277864933014 at step: 228 lr 0.0001\n",
      "2024_06_11_06_02_56 Train loss: 0.24081894755363464 at step: 229 lr 0.0001\n",
      "2024_06_11_06_02_57 Train loss: 0.3216181993484497 at step: 230 lr 0.0001\n",
      "2024_06_11_06_02_58 Train loss: 0.3134613633155823 at step: 231 lr 0.0001\n",
      "2024_06_11_06_02_59 Train loss: 0.33181726932525635 at step: 232 lr 0.0001\n",
      "2024_06_11_06_03_00 Train loss: 0.35301804542541504 at step: 233 lr 0.0001\n",
      "2024_06_11_06_03_01 Train loss: 0.2074528932571411 at step: 234 lr 0.0001\n",
      "2024_06_11_06_03_02 Train loss: 0.28447088599205017 at step: 235 lr 0.0001\n",
      "2024_06_11_06_03_04 Train loss: 0.3141011595726013 at step: 236 lr 0.0001\n",
      "2024_06_11_06_03_05 Train loss: 0.3377770483493805 at step: 237 lr 0.0001\n",
      "2024_06_11_06_03_06 Train loss: 0.21228641271591187 at step: 238 lr 0.0001\n",
      "2024_06_11_06_03_07 Train loss: 0.3031802177429199 at step: 239 lr 0.0001\n",
      "2024_06_11_06_03_09 Train loss: 0.2042909860610962 at step: 240 lr 0.0001\n",
      "2024_06_11_06_03_10 Train loss: 0.3080943822860718 at step: 241 lr 0.0001\n",
      "2024_06_11_06_03_11 Train loss: 0.28494080901145935 at step: 242 lr 0.0001\n",
      "2024_06_11_06_03_13 Train loss: 0.5224924683570862 at step: 243 lr 0.0001\n",
      "2024_06_11_06_03_13 Train loss: 0.3370855450630188 at step: 244 lr 0.0001\n",
      "2024_06_11_06_03_14 Train loss: 0.29676568508148193 at step: 245 lr 0.0001\n",
      "2024_06_11_06_03_16 Train loss: 0.4365847706794739 at step: 246 lr 0.0001\n",
      "2024_06_11_06_03_18 Train loss: 0.16750884056091309 at step: 247 lr 0.0001\n",
      "2024_06_11_06_03_19 Train loss: 0.33657142519950867 at step: 248 lr 0.0001\n",
      "2024_06_11_06_03_21 Train loss: 0.21454453468322754 at step: 249 lr 0.0001\n",
      "2024_06_11_06_03_22 Train loss: 0.26716703176498413 at step: 250 lr 0.0001\n",
      "2024_06_11_06_03_23 Train loss: 0.37737518548965454 at step: 251 lr 0.0001\n",
      "2024_06_11_06_03_25 Train loss: 0.17314797639846802 at step: 252 lr 0.0001\n",
      "2024_06_11_06_03_26 Train loss: 0.17615371942520142 at step: 253 lr 0.0001\n",
      "2024_06_11_06_03_27 Train loss: 0.461813747882843 at step: 254 lr 0.0001\n",
      "2024_06_11_06_03_28 Train loss: 0.29258766770362854 at step: 255 lr 0.0001\n",
      "2024_06_11_06_03_29 Train loss: 0.4240093231201172 at step: 256 lr 0.0001\n",
      "2024_06_11_06_03_30 Train loss: 0.29085999727249146 at step: 257 lr 0.0001\n",
      "2024_06_11_06_03_32 Train loss: 0.3735540509223938 at step: 258 lr 0.0001\n",
      "2024_06_11_06_03_32 Train loss: 0.33696362376213074 at step: 259 lr 0.0001\n",
      "2024_06_11_06_03_34 Train loss: 0.21680229902267456 at step: 260 lr 0.0001\n",
      "2024_06_11_06_03_36 Train loss: 0.5262354612350464 at step: 261 lr 0.0001\n",
      "2024_06_11_06_03_38 Train loss: 0.3279867172241211 at step: 262 lr 0.0001\n",
      "2024_06_11_06_03_39 Train loss: 0.2449938803911209 at step: 263 lr 0.0001\n",
      "2024_06_11_06_03_40 Train loss: 0.3406274616718292 at step: 264 lr 0.0001\n",
      "2024_06_11_06_03_41 Train loss: 0.30209121108055115 at step: 265 lr 0.0001\n",
      "2024_06_11_06_03_43 Train loss: 0.30286210775375366 at step: 266 lr 0.0001\n",
      "2024_06_11_06_03_44 Train loss: 0.39511507749557495 at step: 267 lr 0.0001\n",
      "2024_06_11_06_03_46 Train loss: 0.42093563079833984 at step: 268 lr 0.0001\n",
      "2024_06_11_06_03_46 Train loss: 0.2741530239582062 at step: 269 lr 0.0001\n",
      "2024_06_11_06_03_48 Train loss: 0.3237738013267517 at step: 270 lr 0.0001\n",
      "2024_06_11_06_03_50 Train loss: 0.28542694449424744 at step: 271 lr 0.0001\n",
      "2024_06_11_06_03_51 Train loss: 0.2775968909263611 at step: 272 lr 0.0001\n",
      "2024_06_11_06_03_56 Train loss: 0.5510247945785522 at step: 273 lr 0.0001\n",
      "2024_06_11_06_03_58 Train loss: 0.30044955015182495 at step: 274 lr 0.0001\n",
      "2024_06_11_06_04_02 Train loss: 0.3304862976074219 at step: 275 lr 0.0001\n",
      "2024_06_11_06_04_03 Train loss: 0.45095473527908325 at step: 276 lr 0.0001\n",
      "2024_06_11_06_04_05 Train loss: 0.4722917675971985 at step: 277 lr 0.0001\n",
      "2024_06_11_06_04_06 Train loss: 0.2609335780143738 at step: 278 lr 0.0001\n",
      "2024_06_11_06_04_07 Train loss: 0.2831147015094757 at step: 279 lr 0.0001\n",
      "2024_06_11_06_04_10 Train loss: 0.380173921585083 at step: 280 lr 0.0001\n",
      "2024_06_11_06_04_11 Train loss: 0.1884838044643402 at step: 281 lr 0.0001\n",
      "2024_06_11_06_04_12 Train loss: 0.281554251909256 at step: 282 lr 0.0001\n",
      "2024_06_11_06_04_14 Train loss: 0.2678309679031372 at step: 283 lr 0.0001\n",
      "2024_06_11_06_04_15 Train loss: 0.26513606309890747 at step: 284 lr 0.0001\n",
      "2024_06_11_06_04_16 Train loss: 0.27772727608680725 at step: 285 lr 0.0001\n",
      "2024_06_11_06_04_18 Train loss: 0.24137841165065765 at step: 286 lr 0.0001\n",
      "2024_06_11_06_04_20 Train loss: 0.5931086540222168 at step: 287 lr 0.0001\n",
      "2024_06_11_06_04_21 Train loss: 0.232020765542984 at step: 288 lr 0.0001\n",
      "2024_06_11_06_04_22 Train loss: 0.2593369781970978 at step: 289 lr 0.0001\n",
      "2024_06_11_06_04_23 Train loss: 0.30660417675971985 at step: 290 lr 0.0001\n",
      "2024_06_11_06_04_24 Train loss: 0.30611079931259155 at step: 291 lr 0.0001\n",
      "2024_06_11_06_04_25 Train loss: 0.31879130005836487 at step: 292 lr 0.0001\n",
      "2024_06_11_06_04_27 Train loss: 0.1558210253715515 at step: 293 lr 0.0001\n",
      "2024_06_11_06_04_28 Train loss: 0.1715233474969864 at step: 294 lr 0.0001\n",
      "2024_06_11_06_04_29 Train loss: 0.49438613653182983 at step: 295 lr 0.0001\n",
      "2024_06_11_06_04_30 Train loss: 0.23826801776885986 at step: 296 lr 0.0001\n",
      "(Val @ epoch 3) acc: 0.8809523809523809; ap: 0.9764332674161289\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_06_04_45 Train loss: 0.33296024799346924 at step: 297 lr 0.0001\n",
      "2024_06_11_06_04_47 Train loss: 0.5361903309822083 at step: 298 lr 0.0001\n",
      "2024_06_11_06_04_49 Train loss: 0.3646351993083954 at step: 299 lr 0.0001\n",
      "2024_06_11_06_04_50 Train loss: 0.43208444118499756 at step: 300 lr 0.0001\n",
      "2024_06_11_06_04_52 Train loss: 0.2420295923948288 at step: 301 lr 0.0001\n",
      "2024_06_11_06_04_54 Train loss: 0.3765833377838135 at step: 302 lr 0.0001\n",
      "2024_06_11_06_04_55 Train loss: 0.22293701767921448 at step: 303 lr 0.0001\n",
      "2024_06_11_06_04_56 Train loss: 0.21094967424869537 at step: 304 lr 0.0001\n",
      "2024_06_11_06_04_57 Train loss: 0.7111830711364746 at step: 305 lr 0.0001\n",
      "2024_06_11_06_04_58 Train loss: 0.341777503490448 at step: 306 lr 0.0001\n",
      "2024_06_11_06_04_59 Train loss: 0.23958836495876312 at step: 307 lr 0.0001\n",
      "2024_06_11_06_05_00 Train loss: 0.25665128231048584 at step: 308 lr 0.0001\n",
      "2024_06_11_06_05_01 Train loss: 0.21345937252044678 at step: 309 lr 0.0001\n",
      "2024_06_11_06_05_03 Train loss: 0.18998685479164124 at step: 310 lr 0.0001\n",
      "2024_06_11_06_05_03 Train loss: 0.21970663964748383 at step: 311 lr 0.0001\n",
      "2024_06_11_06_05_04 Train loss: 0.2342870682477951 at step: 312 lr 0.0001\n",
      "2024_06_11_06_05_05 Train loss: 0.29740428924560547 at step: 313 lr 0.0001\n",
      "2024_06_11_06_05_06 Train loss: 0.5046594738960266 at step: 314 lr 0.0001\n",
      "2024_06_11_06_05_07 Train loss: 0.1875886619091034 at step: 315 lr 0.0001\n",
      "2024_06_11_06_05_08 Train loss: 0.34022170305252075 at step: 316 lr 0.0001\n",
      "2024_06_11_06_05_09 Train loss: 0.38985878229141235 at step: 317 lr 0.0001\n",
      "2024_06_11_06_05_10 Train loss: 0.45026618242263794 at step: 318 lr 0.0001\n",
      "2024_06_11_06_05_11 Train loss: 0.20343534648418427 at step: 319 lr 0.0001\n",
      "2024_06_11_06_05_12 Train loss: 0.3335558772087097 at step: 320 lr 0.0001\n",
      "2024_06_11_06_05_13 Train loss: 0.35020512342453003 at step: 321 lr 0.0001\n",
      "2024_06_11_06_05_14 Train loss: 0.1492459625005722 at step: 322 lr 0.0001\n",
      "2024_06_11_06_05_15 Train loss: 0.35658198595046997 at step: 323 lr 0.0001\n",
      "2024_06_11_06_05_16 Train loss: 0.19771189987659454 at step: 324 lr 0.0001\n",
      "2024_06_11_06_05_17 Train loss: 0.23589542508125305 at step: 325 lr 0.0001\n",
      "2024_06_11_06_05_19 Train loss: 0.22545656561851501 at step: 326 lr 0.0001\n",
      "2024_06_11_06_05_20 Train loss: 0.25062233209609985 at step: 327 lr 0.0001\n",
      "2024_06_11_06_05_20 Train loss: 0.19643424451351166 at step: 328 lr 0.0001\n",
      "2024_06_11_06_05_22 Train loss: 0.4182111322879791 at step: 329 lr 0.0001\n",
      "2024_06_11_06_05_23 Train loss: 0.3082003593444824 at step: 330 lr 0.0001\n",
      "2024_06_11_06_05_25 Train loss: 0.18282563984394073 at step: 331 lr 0.0001\n",
      "2024_06_11_06_05_25 Train loss: 0.24961400032043457 at step: 332 lr 0.0001\n",
      "2024_06_11_06_05_26 Train loss: 0.31202995777130127 at step: 333 lr 0.0001\n",
      "2024_06_11_06_05_28 Train loss: 0.2660498023033142 at step: 334 lr 0.0001\n",
      "2024_06_11_06_05_28 Train loss: 0.21428637206554413 at step: 335 lr 0.0001\n",
      "2024_06_11_06_05_31 Train loss: 0.22537824511528015 at step: 336 lr 0.0001\n",
      "2024_06_11_06_05_32 Train loss: 0.35583940148353577 at step: 337 lr 0.0001\n",
      "2024_06_11_06_05_33 Train loss: 0.40877723693847656 at step: 338 lr 0.0001\n",
      "2024_06_11_06_05_34 Train loss: 0.27607595920562744 at step: 339 lr 0.0001\n",
      "2024_06_11_06_05_35 Train loss: 0.43349331617355347 at step: 340 lr 0.0001\n",
      "2024_06_11_06_05_36 Train loss: 0.15085412561893463 at step: 341 lr 0.0001\n",
      "2024_06_11_06_05_37 Train loss: 0.22269925475120544 at step: 342 lr 0.0001\n",
      "2024_06_11_06_05_39 Train loss: 0.30813151597976685 at step: 343 lr 0.0001\n",
      "2024_06_11_06_05_40 Train loss: 0.34902718663215637 at step: 344 lr 0.0001\n",
      "2024_06_11_06_05_41 Train loss: 0.27117791771888733 at step: 345 lr 0.0001\n",
      "2024_06_11_06_05_42 Train loss: 0.30746322870254517 at step: 346 lr 0.0001\n",
      "2024_06_11_06_05_44 Train loss: 0.18356896936893463 at step: 347 lr 0.0001\n",
      "2024_06_11_06_05_45 Train loss: 0.3972715735435486 at step: 348 lr 0.0001\n",
      "2024_06_11_06_05_47 Train loss: 0.36754077672958374 at step: 349 lr 0.0001\n",
      "2024_06_11_06_05_48 Train loss: 0.14582544565200806 at step: 350 lr 0.0001\n",
      "2024_06_11_06_05_49 Train loss: 0.25965091586112976 at step: 351 lr 0.0001\n",
      "2024_06_11_06_05_50 Train loss: 0.34545645117759705 at step: 352 lr 0.0001\n",
      "2024_06_11_06_05_51 Train loss: 0.22758817672729492 at step: 353 lr 0.0001\n",
      "2024_06_11_06_05_52 Train loss: 0.3444952964782715 at step: 354 lr 0.0001\n",
      "2024_06_11_06_05_54 Train loss: 0.292695015668869 at step: 355 lr 0.0001\n",
      "2024_06_11_06_05_55 Train loss: 0.3058773875236511 at step: 356 lr 0.0001\n",
      "2024_06_11_06_05_56 Train loss: 0.163678377866745 at step: 357 lr 0.0001\n",
      "2024_06_11_06_05_57 Train loss: 0.11272761225700378 at step: 358 lr 0.0001\n",
      "2024_06_11_06_05_58 Train loss: 0.379130482673645 at step: 359 lr 0.0001\n",
      "2024_06_11_06_05_59 Train loss: 0.23592165112495422 at step: 360 lr 0.0001\n",
      "2024_06_11_06_06_01 Train loss: 0.21048879623413086 at step: 361 lr 0.0001\n",
      "2024_06_11_06_06_02 Train loss: 0.1616729199886322 at step: 362 lr 0.0001\n",
      "2024_06_11_06_06_03 Train loss: 0.40313729643821716 at step: 363 lr 0.0001\n",
      "2024_06_11_06_06_04 Train loss: 0.15368640422821045 at step: 364 lr 0.0001\n",
      "2024_06_11_06_06_06 Train loss: 0.2651435136795044 at step: 365 lr 0.0001\n",
      "2024_06_11_06_06_07 Train loss: 0.28496691584587097 at step: 366 lr 0.0001\n",
      "2024_06_11_06_06_09 Train loss: 0.47259509563446045 at step: 367 lr 0.0001\n",
      "2024_06_11_06_06_10 Train loss: 0.18420350551605225 at step: 368 lr 0.0001\n",
      "2024_06_11_06_06_12 Train loss: 0.2943883240222931 at step: 369 lr 0.0001\n",
      "2024_06_11_06_06_12 Train loss: 0.43271347880363464 at step: 370 lr 0.0001\n",
      "(Val @ epoch 4) acc: 0.8231292517006803; ap: 0.9291653368793757\n",
      "2024_06_11_06_06_31 Train loss: 0.2686283588409424 at step: 371 lr 0.0001\n",
      "2024_06_11_06_06_32 Train loss: 0.21372082829475403 at step: 372 lr 0.0001\n",
      "2024_06_11_06_06_33 Train loss: 0.20619156956672668 at step: 373 lr 0.0001\n",
      "2024_06_11_06_06_34 Train loss: 0.24422606825828552 at step: 374 lr 0.0001\n",
      "2024_06_11_06_06_36 Train loss: 0.24346378445625305 at step: 375 lr 0.0001\n",
      "2024_06_11_06_06_37 Train loss: 0.19971536099910736 at step: 376 lr 0.0001\n",
      "2024_06_11_06_06_38 Train loss: 0.39931100606918335 at step: 377 lr 0.0001\n",
      "2024_06_11_06_06_42 Train loss: 0.31176918745040894 at step: 378 lr 0.0001\n",
      "2024_06_11_06_06_43 Train loss: 0.282270222902298 at step: 379 lr 0.0001\n",
      "2024_06_11_06_06_44 Train loss: 0.1562335044145584 at step: 380 lr 0.0001\n",
      "2024_06_11_06_06_45 Train loss: 0.32123231887817383 at step: 381 lr 0.0001\n",
      "2024_06_11_06_06_46 Train loss: 0.2861868143081665 at step: 382 lr 0.0001\n",
      "2024_06_11_06_06_48 Train loss: 0.51244056224823 at step: 383 lr 0.0001\n",
      "2024_06_11_06_06_50 Train loss: 0.2947116494178772 at step: 384 lr 0.0001\n",
      "2024_06_11_06_06_51 Train loss: 0.35470467805862427 at step: 385 lr 0.0001\n",
      "2024_06_11_06_06_52 Train loss: 0.3214647173881531 at step: 386 lr 0.0001\n",
      "2024_06_11_06_06_53 Train loss: 0.4219614565372467 at step: 387 lr 0.0001\n",
      "2024_06_11_06_06_55 Train loss: 0.16707220673561096 at step: 388 lr 0.0001\n",
      "2024_06_11_06_06_57 Train loss: 0.2611042261123657 at step: 389 lr 0.0001\n",
      "2024_06_11_06_06_59 Train loss: 0.23280149698257446 at step: 390 lr 0.0001\n",
      "2024_06_11_06_07_00 Train loss: 0.14156728982925415 at step: 391 lr 0.0001\n",
      "2024_06_11_06_07_02 Train loss: 0.207535058259964 at step: 392 lr 0.0001\n",
      "2024_06_11_06_07_04 Train loss: 0.2180858999490738 at step: 393 lr 0.0001\n",
      "2024_06_11_06_07_06 Train loss: 0.4110327959060669 at step: 394 lr 0.0001\n",
      "2024_06_11_06_07_07 Train loss: 0.18018829822540283 at step: 395 lr 0.0001\n",
      "2024_06_11_06_07_08 Train loss: 0.22651013731956482 at step: 396 lr 0.0001\n",
      "2024_06_11_06_07_09 Train loss: 0.15216225385665894 at step: 397 lr 0.0001\n",
      "2024_06_11_06_07_11 Train loss: 0.3334707021713257 at step: 398 lr 0.0001\n",
      "2024_06_11_06_07_13 Train loss: 0.26365530490875244 at step: 399 lr 0.0001\n",
      "2024_06_11_06_07_14 Train loss: 0.34334230422973633 at step: 400 lr 0.0001\n",
      "2024_06_11_06_07_15 Train loss: 0.15626481175422668 at step: 401 lr 0.0001\n",
      "2024_06_11_06_07_17 Train loss: 0.30123278498649597 at step: 402 lr 0.0001\n",
      "2024_06_11_06_07_18 Train loss: 0.29728254675865173 at step: 403 lr 0.0001\n",
      "2024_06_11_06_07_19 Train loss: 0.29462867975234985 at step: 404 lr 0.0001\n",
      "2024_06_11_06_07_20 Train loss: 0.21838057041168213 at step: 405 lr 0.0001\n",
      "2024_06_11_06_07_21 Train loss: 0.2607073187828064 at step: 406 lr 0.0001\n",
      "2024_06_11_06_07_24 Train loss: 0.33628368377685547 at step: 407 lr 0.0001\n",
      "2024_06_11_06_07_24 Train loss: 0.3246232271194458 at step: 408 lr 0.0001\n",
      "2024_06_11_06_07_26 Train loss: 0.22300365567207336 at step: 409 lr 0.0001\n",
      "2024_06_11_06_07_27 Train loss: 0.1765318512916565 at step: 410 lr 0.0001\n",
      "2024_06_11_06_07_27 Train loss: 0.33718156814575195 at step: 411 lr 0.0001\n",
      "2024_06_11_06_07_28 Train loss: 0.29836905002593994 at step: 412 lr 0.0001\n",
      "2024_06_11_06_07_30 Train loss: 0.18251001834869385 at step: 413 lr 0.0001\n",
      "2024_06_11_06_07_31 Train loss: 0.14544063806533813 at step: 414 lr 0.0001\n",
      "2024_06_11_06_07_32 Train loss: 0.14017635583877563 at step: 415 lr 0.0001\n",
      "2024_06_11_06_07_32 Train loss: 0.352189838886261 at step: 416 lr 0.0001\n",
      "2024_06_11_06_07_34 Train loss: 0.1287800371646881 at step: 417 lr 0.0001\n",
      "2024_06_11_06_07_35 Train loss: 0.3034481704235077 at step: 418 lr 0.0001\n",
      "2024_06_11_06_07_36 Train loss: 0.27982157468795776 at step: 419 lr 0.0001\n",
      "2024_06_11_06_07_37 Train loss: 0.11984742432832718 at step: 420 lr 0.0001\n",
      "2024_06_11_06_07_38 Train loss: 0.16590939462184906 at step: 421 lr 0.0001\n",
      "2024_06_11_06_07_39 Train loss: 0.1837385892868042 at step: 422 lr 0.0001\n",
      "2024_06_11_06_07_41 Train loss: 0.19449162483215332 at step: 423 lr 0.0001\n",
      "2024_06_11_06_07_43 Train loss: 0.32151198387145996 at step: 424 lr 0.0001\n",
      "2024_06_11_06_07_44 Train loss: 0.3116757273674011 at step: 425 lr 0.0001\n",
      "2024_06_11_06_07_45 Train loss: 0.3798714280128479 at step: 426 lr 0.0001\n",
      "2024_06_11_06_07_47 Train loss: 0.35009247064590454 at step: 427 lr 0.0001\n",
      "2024_06_11_06_07_49 Train loss: 0.18341296911239624 at step: 428 lr 0.0001\n",
      "2024_06_11_06_07_50 Train loss: 0.2872146964073181 at step: 429 lr 0.0001\n",
      "2024_06_11_06_07_51 Train loss: 0.16578418016433716 at step: 430 lr 0.0001\n",
      "2024_06_11_06_07_52 Train loss: 0.21247856318950653 at step: 431 lr 0.0001\n",
      "2024_06_11_06_07_54 Train loss: 0.365686297416687 at step: 432 lr 0.0001\n",
      "2024_06_11_06_07_57 Train loss: 0.15852677822113037 at step: 433 lr 0.0001\n",
      "2024_06_11_06_08_02 Train loss: 0.2985650897026062 at step: 434 lr 0.0001\n",
      "2024_06_11_06_08_04 Train loss: 0.163184255361557 at step: 435 lr 0.0001\n",
      "2024_06_11_06_08_06 Train loss: 0.10880446434020996 at step: 436 lr 0.0001\n",
      "2024_06_11_06_08_10 Train loss: 0.2643282413482666 at step: 437 lr 0.0001\n",
      "2024_06_11_06_08_14 Train loss: 0.32209956645965576 at step: 438 lr 0.0001\n",
      "2024_06_11_06_08_17 Train loss: 0.28961679339408875 at step: 439 lr 0.0001\n",
      "2024_06_11_06_08_19 Train loss: 0.2579873502254486 at step: 440 lr 0.0001\n",
      "2024_06_11_06_08_20 Train loss: 0.3709363341331482 at step: 441 lr 0.0001\n",
      "2024_06_11_06_08_21 Train loss: 0.15474078059196472 at step: 442 lr 0.0001\n",
      "2024_06_11_06_08_22 Train loss: 0.3297260105609894 at step: 443 lr 0.0001\n",
      "2024_06_11_06_08_23 Train loss: 0.46067774295806885 at step: 444 lr 0.0001\n",
      "(Val @ epoch 5) acc: 0.8571428571428571; ap: 0.9392392825181384\n",
      "2024_06_11_06_08_42 Train loss: 0.28749099373817444 at step: 445 lr 0.0001\n",
      "2024_06_11_06_08_43 Train loss: 0.35511794686317444 at step: 446 lr 0.0001\n",
      "2024_06_11_06_08_45 Train loss: 0.2657105624675751 at step: 447 lr 0.0001\n",
      "2024_06_11_06_08_45 Train loss: 0.14616425335407257 at step: 448 lr 0.0001\n",
      "2024_06_11_06_08_47 Train loss: 0.32475316524505615 at step: 449 lr 0.0001\n",
      "2024_06_11_06_08_48 Train loss: 0.11801479756832123 at step: 450 lr 0.0001\n",
      "2024_06_11_06_08_49 Train loss: 0.45224863290786743 at step: 451 lr 0.0001\n",
      "2024_06_11_06_08_51 Train loss: 0.2738816738128662 at step: 452 lr 0.0001\n",
      "2024_06_11_06_08_52 Train loss: 0.39038410782814026 at step: 453 lr 0.0001\n",
      "2024_06_11_06_08_52 Train loss: 0.23873141407966614 at step: 454 lr 0.0001\n",
      "2024_06_11_06_08_53 Train loss: 0.24077960848808289 at step: 455 lr 0.0001\n",
      "2024_06_11_06_08_55 Train loss: 0.21318379044532776 at step: 456 lr 0.0001\n",
      "2024_06_11_06_08_56 Train loss: 0.34207943081855774 at step: 457 lr 0.0001\n",
      "2024_06_11_06_08_57 Train loss: 0.16134954988956451 at step: 458 lr 0.0001\n",
      "2024_06_11_06_08_58 Train loss: 0.4500250220298767 at step: 459 lr 0.0001\n",
      "2024_06_11_06_09_00 Train loss: 0.26593586802482605 at step: 460 lr 0.0001\n",
      "2024_06_11_06_09_02 Train loss: 0.28607162833213806 at step: 461 lr 0.0001\n",
      "2024_06_11_06_09_04 Train loss: 0.4180968403816223 at step: 462 lr 0.0001\n",
      "2024_06_11_06_09_04 Train loss: 0.24729225039482117 at step: 463 lr 0.0001\n",
      "2024_06_11_06_09_05 Train loss: 0.2592944800853729 at step: 464 lr 0.0001\n",
      "2024_06_11_06_09_06 Train loss: 0.3045468330383301 at step: 465 lr 0.0001\n",
      "2024_06_11_06_09_07 Train loss: 0.4109070897102356 at step: 466 lr 0.0001\n",
      "2024_06_11_06_09_09 Train loss: 0.22477582097053528 at step: 467 lr 0.0001\n",
      "2024_06_11_06_09_11 Train loss: 0.2613733410835266 at step: 468 lr 0.0001\n",
      "2024_06_11_06_09_13 Train loss: 0.46926191449165344 at step: 469 lr 0.0001\n",
      "2024_06_11_06_09_14 Train loss: 0.19029134511947632 at step: 470 lr 0.0001\n",
      "2024_06_11_06_09_16 Train loss: 0.25205761194229126 at step: 471 lr 0.0001\n",
      "2024_06_11_06_09_17 Train loss: 0.22101739048957825 at step: 472 lr 0.0001\n",
      "2024_06_11_06_09_18 Train loss: 0.31247299909591675 at step: 473 lr 0.0001\n",
      "2024_06_11_06_09_19 Train loss: 0.2813308835029602 at step: 474 lr 0.0001\n",
      "2024_06_11_06_09_20 Train loss: 0.33821165561676025 at step: 475 lr 0.0001\n",
      "2024_06_11_06_09_21 Train loss: 0.19231116771697998 at step: 476 lr 0.0001\n",
      "2024_06_11_06_09_23 Train loss: 0.25329774618148804 at step: 477 lr 0.0001\n",
      "2024_06_11_06_09_24 Train loss: 0.22931744158267975 at step: 478 lr 0.0001\n",
      "2024_06_11_06_09_26 Train loss: 0.33910810947418213 at step: 479 lr 0.0001\n",
      "2024_06_11_06_09_27 Train loss: 0.211037278175354 at step: 480 lr 0.0001\n",
      "2024_06_11_06_09_29 Train loss: 0.2744221091270447 at step: 481 lr 0.0001\n",
      "2024_06_11_06_09_31 Train loss: 0.1540919840335846 at step: 482 lr 0.0001\n",
      "2024_06_11_06_09_32 Train loss: 0.31703218817710876 at step: 483 lr 0.0001\n",
      "2024_06_11_06_09_33 Train loss: 0.14856241643428802 at step: 484 lr 0.0001\n",
      "2024_06_11_06_09_34 Train loss: 0.09354040771722794 at step: 485 lr 0.0001\n",
      "2024_06_11_06_09_36 Train loss: 0.5552829504013062 at step: 486 lr 0.0001\n",
      "2024_06_11_06_09_39 Train loss: 0.2594256103038788 at step: 487 lr 0.0001\n",
      "2024_06_11_06_09_40 Train loss: 0.2573104500770569 at step: 488 lr 0.0001\n",
      "2024_06_11_06_09_41 Train loss: 0.19077128171920776 at step: 489 lr 0.0001\n",
      "2024_06_11_06_09_43 Train loss: 0.26621121168136597 at step: 490 lr 0.0001\n",
      "2024_06_11_06_09_44 Train loss: 0.33976811170578003 at step: 491 lr 0.0001\n",
      "2024_06_11_06_09_46 Train loss: 0.22728684544563293 at step: 492 lr 0.0001\n",
      "2024_06_11_06_09_47 Train loss: 0.48804640769958496 at step: 493 lr 0.0001\n",
      "2024_06_11_06_09_48 Train loss: 0.34565362334251404 at step: 494 lr 0.0001\n",
      "2024_06_11_06_09_49 Train loss: 0.2806081473827362 at step: 495 lr 0.0001\n",
      "2024_06_11_06_09_50 Train loss: 0.28820133209228516 at step: 496 lr 0.0001\n",
      "2024_06_11_06_09_51 Train loss: 0.18636183440685272 at step: 497 lr 0.0001\n",
      "2024_06_11_06_09_52 Train loss: 0.24355927109718323 at step: 498 lr 0.0001\n",
      "2024_06_11_06_09_53 Train loss: 0.19533635675907135 at step: 499 lr 0.0001\n",
      "2024_06_11_06_09_54 Train loss: 0.11772400140762329 at step: 500 lr 0.0001\n",
      "2024_06_11_06_09_55 Train loss: 0.2485223412513733 at step: 501 lr 0.0001\n",
      "2024_06_11_06_09_57 Train loss: 0.4461038112640381 at step: 502 lr 0.0001\n",
      "2024_06_11_06_09_58 Train loss: 0.1436416506767273 at step: 503 lr 0.0001\n",
      "2024_06_11_06_09_59 Train loss: 0.16626521944999695 at step: 504 lr 0.0001\n",
      "2024_06_11_06_10_00 Train loss: 0.5005080699920654 at step: 505 lr 0.0001\n",
      "2024_06_11_06_10_01 Train loss: 0.20051074028015137 at step: 506 lr 0.0001\n",
      "2024_06_11_06_10_03 Train loss: 0.341070294380188 at step: 507 lr 0.0001\n",
      "2024_06_11_06_10_05 Train loss: 0.1669469028711319 at step: 508 lr 0.0001\n",
      "2024_06_11_06_10_06 Train loss: 0.18107563257217407 at step: 509 lr 0.0001\n",
      "2024_06_11_06_10_08 Train loss: 0.20309565961360931 at step: 510 lr 0.0001\n",
      "2024_06_11_06_10_09 Train loss: 0.241235613822937 at step: 511 lr 0.0001\n",
      "2024_06_11_06_10_10 Train loss: 0.4196680188179016 at step: 512 lr 0.0001\n",
      "2024_06_11_06_10_12 Train loss: 0.3405795693397522 at step: 513 lr 0.0001\n",
      "2024_06_11_06_10_13 Train loss: 0.20928283035755157 at step: 514 lr 0.0001\n",
      "2024_06_11_06_10_15 Train loss: 0.347551167011261 at step: 515 lr 0.0001\n",
      "2024_06_11_06_10_16 Train loss: 0.40599459409713745 at step: 516 lr 0.0001\n",
      "2024_06_11_06_10_18 Train loss: 0.23964717984199524 at step: 517 lr 0.0001\n",
      "2024_06_11_06_10_18 Train loss: 0.26557597517967224 at step: 518 lr 0.0001\n",
      "(Val @ epoch 6) acc: 0.8775510204081632; ap: 0.9484490797814679\n",
      "2024_06_11_06_10_38 Train loss: 0.34016716480255127 at step: 519 lr 0.0001\n",
      "2024_06_11_06_10_38 Train loss: 0.1200764924287796 at step: 520 lr 0.0001\n",
      "2024_06_11_06_10_39 Train loss: 0.3984587490558624 at step: 521 lr 0.0001\n",
      "2024_06_11_06_10_40 Train loss: 0.20181438326835632 at step: 522 lr 0.0001\n",
      "2024_06_11_06_10_41 Train loss: 0.3252369463443756 at step: 523 lr 0.0001\n",
      "2024_06_11_06_10_42 Train loss: 0.5896425247192383 at step: 524 lr 0.0001\n",
      "2024_06_11_06_10_43 Train loss: 0.28983765840530396 at step: 525 lr 0.0001\n",
      "2024_06_11_06_10_44 Train loss: 0.32068416476249695 at step: 526 lr 0.0001\n",
      "2024_06_11_06_10_46 Train loss: 0.3309718370437622 at step: 527 lr 0.0001\n",
      "2024_06_11_06_10_48 Train loss: 0.13500629365444183 at step: 528 lr 0.0001\n",
      "2024_06_11_06_10_49 Train loss: 0.15990668535232544 at step: 529 lr 0.0001\n",
      "2024_06_11_06_10_50 Train loss: 0.1668480932712555 at step: 530 lr 0.0001\n",
      "2024_06_11_06_10_51 Train loss: 0.20398518443107605 at step: 531 lr 0.0001\n",
      "2024_06_11_06_10_54 Train loss: 0.27594155073165894 at step: 532 lr 0.0001\n",
      "2024_06_11_06_10_55 Train loss: 0.2768559753894806 at step: 533 lr 0.0001\n",
      "2024_06_11_06_10_55 Train loss: 0.4446645975112915 at step: 534 lr 0.0001\n",
      "2024_06_11_06_10_57 Train loss: 0.3333209455013275 at step: 535 lr 0.0001\n",
      "2024_06_11_06_10_58 Train loss: 0.3108423948287964 at step: 536 lr 0.0001\n",
      "2024_06_11_06_11_00 Train loss: 0.10835783928632736 at step: 537 lr 0.0001\n",
      "2024_06_11_06_11_02 Train loss: 0.12389200180768967 at step: 538 lr 0.0001\n",
      "2024_06_11_06_11_04 Train loss: 0.1967071294784546 at step: 539 lr 0.0001\n",
      "2024_06_11_06_11_06 Train loss: 0.19018597900867462 at step: 540 lr 0.0001\n",
      "2024_06_11_06_11_07 Train loss: 0.3996461033821106 at step: 541 lr 0.0001\n",
      "2024_06_11_06_11_09 Train loss: 0.5348302721977234 at step: 542 lr 0.0001\n",
      "2024_06_11_06_11_11 Train loss: 0.536273717880249 at step: 543 lr 0.0001\n",
      "2024_06_11_06_11_12 Train loss: 0.47462719678878784 at step: 544 lr 0.0001\n",
      "2024_06_11_06_11_12 Train loss: 0.11418949067592621 at step: 545 lr 0.0001\n",
      "2024_06_11_06_11_14 Train loss: 0.2705443203449249 at step: 546 lr 0.0001\n",
      "2024_06_11_06_11_15 Train loss: 0.29663002490997314 at step: 547 lr 0.0001\n",
      "2024_06_11_06_11_17 Train loss: 0.24854755401611328 at step: 548 lr 0.0001\n",
      "2024_06_11_06_11_19 Train loss: 0.3069656491279602 at step: 549 lr 0.0001\n",
      "2024_06_11_06_11_20 Train loss: 0.21660567820072174 at step: 550 lr 0.0001\n",
      "2024_06_11_06_11_21 Train loss: 0.14696723222732544 at step: 551 lr 0.0001\n",
      "2024_06_11_06_11_22 Train loss: 0.2519153952598572 at step: 552 lr 0.0001\n",
      "2024_06_11_06_11_24 Train loss: 0.3455427289009094 at step: 553 lr 0.0001\n",
      "2024_06_11_06_11_26 Train loss: 0.24758552014827728 at step: 554 lr 0.0001\n",
      "2024_06_11_06_11_28 Train loss: 0.692783534526825 at step: 555 lr 0.0001\n",
      "2024_06_11_06_11_29 Train loss: 0.378558874130249 at step: 556 lr 0.0001\n",
      "2024_06_11_06_11_30 Train loss: 0.45152348279953003 at step: 557 lr 0.0001\n",
      "2024_06_11_06_11_30 Train loss: 0.24045591056346893 at step: 558 lr 0.0001\n",
      "2024_06_11_06_11_32 Train loss: 0.4406778812408447 at step: 559 lr 0.0001\n",
      "2024_06_11_06_11_33 Train loss: 0.2256694883108139 at step: 560 lr 0.0001\n",
      "2024_06_11_06_11_35 Train loss: 0.2907078266143799 at step: 561 lr 0.0001\n",
      "2024_06_11_06_11_36 Train loss: 0.17676988244056702 at step: 562 lr 0.0001\n",
      "2024_06_11_06_11_36 Train loss: 0.30082476139068604 at step: 563 lr 0.0001\n",
      "2024_06_11_06_11_38 Train loss: 0.20218873023986816 at step: 564 lr 0.0001\n",
      "2024_06_11_06_11_39 Train loss: 0.37946605682373047 at step: 565 lr 0.0001\n",
      "2024_06_11_06_11_40 Train loss: 0.35331663489341736 at step: 566 lr 0.0001\n",
      "2024_06_11_06_11_42 Train loss: 0.3799741864204407 at step: 567 lr 0.0001\n",
      "2024_06_11_06_11_43 Train loss: 0.238094300031662 at step: 568 lr 0.0001\n",
      "2024_06_11_06_11_44 Train loss: 0.32440394163131714 at step: 569 lr 0.0001\n",
      "2024_06_11_06_11_45 Train loss: 0.16668373346328735 at step: 570 lr 0.0001\n",
      "2024_06_11_06_11_46 Train loss: 0.27664250135421753 at step: 571 lr 0.0001\n",
      "2024_06_11_06_11_47 Train loss: 0.3278902471065521 at step: 572 lr 0.0001\n",
      "2024_06_11_06_11_47 Train loss: 0.19258204102516174 at step: 573 lr 0.0001\n",
      "2024_06_11_06_11_48 Train loss: 0.12474121153354645 at step: 574 lr 0.0001\n",
      "2024_06_11_06_11_49 Train loss: 0.26552510261535645 at step: 575 lr 0.0001\n",
      "2024_06_11_06_11_50 Train loss: 0.1351187378168106 at step: 576 lr 0.0001\n",
      "2024_06_11_06_11_51 Train loss: 0.17662423849105835 at step: 577 lr 0.0001\n",
      "2024_06_11_06_11_52 Train loss: 0.1840786337852478 at step: 578 lr 0.0001\n",
      "2024_06_11_06_11_54 Train loss: 0.28851625323295593 at step: 579 lr 0.0001\n",
      "2024_06_11_06_11_55 Train loss: 0.32719340920448303 at step: 580 lr 0.0001\n",
      "2024_06_11_06_11_56 Train loss: 0.1620224565267563 at step: 581 lr 0.0001\n",
      "2024_06_11_06_11_57 Train loss: 0.2315170168876648 at step: 582 lr 0.0001\n",
      "2024_06_11_06_11_58 Train loss: 0.24749448895454407 at step: 583 lr 0.0001\n",
      "2024_06_11_06_11_59 Train loss: 0.2067990005016327 at step: 584 lr 0.0001\n",
      "2024_06_11_06_12_01 Train loss: 0.27339696884155273 at step: 585 lr 0.0001\n",
      "2024_06_11_06_12_03 Train loss: 0.42396968603134155 at step: 586 lr 0.0001\n",
      "2024_06_11_06_12_04 Train loss: 0.3885325789451599 at step: 587 lr 0.0001\n",
      "2024_06_11_06_12_07 Train loss: 0.21680399775505066 at step: 588 lr 0.0001\n",
      "2024_06_11_06_12_10 Train loss: 0.2387273758649826 at step: 589 lr 0.0001\n",
      "2024_06_11_06_12_10 Train loss: 0.20738795399665833 at step: 590 lr 0.0001\n",
      "2024_06_11_06_12_13 Train loss: 0.1969323754310608 at step: 591 lr 0.0001\n",
      "2024_06_11_06_12_13 Train loss: 0.13416463136672974 at step: 592 lr 0.0001\n",
      "(Val @ epoch 7) acc: 0.9283276450511946; ap: 0.9695184599583372\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_06_12_26 Train loss: 0.2554611563682556 at step: 593 lr 0.0001\n",
      "2024_06_11_06_12_27 Train loss: 0.2559107542037964 at step: 594 lr 0.0001\n",
      "2024_06_11_06_12_29 Train loss: 0.35920384526252747 at step: 595 lr 0.0001\n",
      "2024_06_11_06_12_31 Train loss: 0.23663479089736938 at step: 596 lr 0.0001\n",
      "2024_06_11_06_12_32 Train loss: 0.5447065830230713 at step: 597 lr 0.0001\n",
      "2024_06_11_06_12_33 Train loss: 0.3116515874862671 at step: 598 lr 0.0001\n",
      "2024_06_11_06_12_34 Train loss: 0.2847200036048889 at step: 599 lr 0.0001\n",
      "2024_06_11_06_12_36 Train loss: 0.25802358984947205 at step: 600 lr 0.0001\n",
      "2024_06_11_06_12_37 Train loss: 0.38782984018325806 at step: 601 lr 0.0001\n",
      "2024_06_11_06_12_38 Train loss: 0.24442964792251587 at step: 602 lr 0.0001\n",
      "2024_06_11_06_12_39 Train loss: 0.16229943931102753 at step: 603 lr 0.0001\n",
      "2024_06_11_06_12_41 Train loss: 0.26860764622688293 at step: 604 lr 0.0001\n",
      "2024_06_11_06_12_42 Train loss: 0.37834668159484863 at step: 605 lr 0.0001\n",
      "2024_06_11_06_12_43 Train loss: 0.1957220882177353 at step: 606 lr 0.0001\n",
      "2024_06_11_06_12_44 Train loss: 0.45341843366622925 at step: 607 lr 0.0001\n",
      "2024_06_11_06_12_46 Train loss: 0.23522931337356567 at step: 608 lr 0.0001\n",
      "2024_06_11_06_12_46 Train loss: 0.21123960614204407 at step: 609 lr 0.0001\n",
      "2024_06_11_06_12_49 Train loss: 0.3581390380859375 at step: 610 lr 0.0001\n",
      "2024_06_11_06_12_50 Train loss: 0.2337280511856079 at step: 611 lr 0.0001\n",
      "2024_06_11_06_12_52 Train loss: 0.172726571559906 at step: 612 lr 0.0001\n",
      "2024_06_11_06_12_53 Train loss: 0.21791432797908783 at step: 613 lr 0.0001\n",
      "2024_06_11_06_12_54 Train loss: 0.20978227257728577 at step: 614 lr 0.0001\n",
      "2024_06_11_06_12_54 Train loss: 0.3288366496562958 at step: 615 lr 0.0001\n",
      "2024_06_11_06_12_56 Train loss: 0.1808059811592102 at step: 616 lr 0.0001\n",
      "2024_06_11_06_12_58 Train loss: 0.43376901745796204 at step: 617 lr 0.0001\n",
      "2024_06_11_06_12_59 Train loss: 0.38817259669303894 at step: 618 lr 0.0001\n",
      "2024_06_11_06_13_00 Train loss: 0.14863061904907227 at step: 619 lr 0.0001\n",
      "2024_06_11_06_13_01 Train loss: 0.25354504585266113 at step: 620 lr 0.0001\n",
      "2024_06_11_06_13_03 Train loss: 0.1541045755147934 at step: 621 lr 0.0001\n",
      "2024_06_11_06_13_04 Train loss: 0.4148208498954773 at step: 622 lr 0.0001\n",
      "2024_06_11_06_13_06 Train loss: 0.24264414608478546 at step: 623 lr 0.0001\n",
      "2024_06_11_06_13_07 Train loss: 0.24877367913722992 at step: 624 lr 0.0001\n",
      "2024_06_11_06_13_08 Train loss: 0.18299193680286407 at step: 625 lr 0.0001\n",
      "2024_06_11_06_13_09 Train loss: 0.2424178421497345 at step: 626 lr 0.0001\n",
      "2024_06_11_06_13_10 Train loss: 0.17988936603069305 at step: 627 lr 0.0001\n",
      "2024_06_11_06_13_11 Train loss: 0.25789138674736023 at step: 628 lr 0.0001\n",
      "2024_06_11_06_13_12 Train loss: 0.28055644035339355 at step: 629 lr 0.0001\n",
      "2024_06_11_06_13_15 Train loss: 0.20335516333580017 at step: 630 lr 0.0001\n",
      "2024_06_11_06_13_16 Train loss: 0.12614348530769348 at step: 631 lr 0.0001\n",
      "2024_06_11_06_13_18 Train loss: 0.3419274091720581 at step: 632 lr 0.0001\n",
      "2024_06_11_06_13_19 Train loss: 0.29294437170028687 at step: 633 lr 0.0001\n",
      "2024_06_11_06_13_20 Train loss: 0.11409725248813629 at step: 634 lr 0.0001\n",
      "2024_06_11_06_13_22 Train loss: 0.16673614084720612 at step: 635 lr 0.0001\n",
      "2024_06_11_06_13_23 Train loss: 0.4549330770969391 at step: 636 lr 0.0001\n",
      "2024_06_11_06_13_25 Train loss: 0.25615179538726807 at step: 637 lr 0.0001\n",
      "2024_06_11_06_13_27 Train loss: 0.37074851989746094 at step: 638 lr 0.0001\n",
      "2024_06_11_06_13_28 Train loss: 0.2860594689846039 at step: 639 lr 0.0001\n",
      "2024_06_11_06_13_28 Train loss: 0.24139541387557983 at step: 640 lr 0.0001\n",
      "2024_06_11_06_13_29 Train loss: 0.12859347462654114 at step: 641 lr 0.0001\n",
      "2024_06_11_06_13_30 Train loss: 0.1510053128004074 at step: 642 lr 0.0001\n",
      "2024_06_11_06_13_31 Train loss: 0.19006270170211792 at step: 643 lr 0.0001\n",
      "2024_06_11_06_13_32 Train loss: 0.2996753454208374 at step: 644 lr 0.0001\n",
      "2024_06_11_06_13_34 Train loss: 0.3514110743999481 at step: 645 lr 0.0001\n",
      "2024_06_11_06_13_35 Train loss: 0.14711260795593262 at step: 646 lr 0.0001\n",
      "2024_06_11_06_13_36 Train loss: 0.20664939284324646 at step: 647 lr 0.0001\n",
      "2024_06_11_06_13_37 Train loss: 0.2723187506198883 at step: 648 lr 0.0001\n",
      "2024_06_11_06_13_39 Train loss: 0.5218155980110168 at step: 649 lr 0.0001\n",
      "2024_06_11_06_13_40 Train loss: 0.1537914127111435 at step: 650 lr 0.0001\n",
      "2024_06_11_06_13_41 Train loss: 0.2075650990009308 at step: 651 lr 0.0001\n",
      "2024_06_11_06_13_43 Train loss: 0.3765757977962494 at step: 652 lr 0.0001\n",
      "2024_06_11_06_13_45 Train loss: 0.3056727647781372 at step: 653 lr 0.0001\n",
      "2024_06_11_06_13_46 Train loss: 0.21540392935276031 at step: 654 lr 0.0001\n",
      "2024_06_11_06_13_47 Train loss: 0.21022255718708038 at step: 655 lr 0.0001\n",
      "2024_06_11_06_13_49 Train loss: 0.3839704394340515 at step: 656 lr 0.0001\n",
      "2024_06_11_06_13_50 Train loss: 0.148056760430336 at step: 657 lr 0.0001\n",
      "2024_06_11_06_13_51 Train loss: 0.14973890781402588 at step: 658 lr 0.0001\n",
      "2024_06_11_06_13_51 Train loss: 0.2136124074459076 at step: 659 lr 0.0001\n",
      "2024_06_11_06_13_52 Train loss: 0.17942160367965698 at step: 660 lr 0.0001\n",
      "2024_06_11_06_13_53 Train loss: 0.28580015897750854 at step: 661 lr 0.0001\n",
      "2024_06_11_06_13_54 Train loss: 0.22944074869155884 at step: 662 lr 0.0001\n",
      "2024_06_11_06_13_55 Train loss: 0.07271500676870346 at step: 663 lr 0.0001\n",
      "2024_06_11_06_13_56 Train loss: 0.4186304211616516 at step: 664 lr 0.0001\n",
      "2024_06_11_06_13_57 Train loss: 0.1804739534854889 at step: 665 lr 0.0001\n",
      "2024_06_11_06_13_58 Train loss: 0.049141816794872284 at step: 666 lr 0.0001\n",
      "(Val @ epoch 8) acc: 0.9081632653061225; ap: 0.949277920104521\n",
      "2024_06_11_06_14_17 Train loss: 0.10717414319515228 at step: 667 lr 0.0001\n",
      "2024_06_11_06_14_17 Train loss: 0.22961103916168213 at step: 668 lr 0.0001\n",
      "2024_06_11_06_14_18 Train loss: 0.46788379549980164 at step: 669 lr 0.0001\n",
      "2024_06_11_06_14_20 Train loss: 0.45823022723197937 at step: 670 lr 0.0001\n",
      "2024_06_11_06_14_21 Train loss: 0.3318321704864502 at step: 671 lr 0.0001\n",
      "2024_06_11_06_14_22 Train loss: 0.28991979360580444 at step: 672 lr 0.0001\n",
      "2024_06_11_06_14_24 Train loss: 0.279310405254364 at step: 673 lr 0.0001\n",
      "2024_06_11_06_14_25 Train loss: 0.2807691693305969 at step: 674 lr 0.0001\n",
      "2024_06_11_06_14_26 Train loss: 0.17308752238750458 at step: 675 lr 0.0001\n",
      "2024_06_11_06_14_27 Train loss: 0.12922625243663788 at step: 676 lr 0.0001\n",
      "2024_06_11_06_14_28 Train loss: 0.32865893840789795 at step: 677 lr 0.0001\n",
      "2024_06_11_06_14_29 Train loss: 0.33928942680358887 at step: 678 lr 0.0001\n",
      "2024_06_11_06_14_31 Train loss: 0.27136653661727905 at step: 679 lr 0.0001\n",
      "2024_06_11_06_14_32 Train loss: 0.13598453998565674 at step: 680 lr 0.0001\n",
      "2024_06_11_06_14_34 Train loss: 0.22044306993484497 at step: 681 lr 0.0001\n",
      "2024_06_11_06_14_36 Train loss: 0.18682220578193665 at step: 682 lr 0.0001\n",
      "2024_06_11_06_14_37 Train loss: 0.30761027336120605 at step: 683 lr 0.0001\n",
      "2024_06_11_06_14_38 Train loss: 0.34287792444229126 at step: 684 lr 0.0001\n",
      "2024_06_11_06_14_39 Train loss: 0.11379072070121765 at step: 685 lr 0.0001\n",
      "2024_06_11_06_14_41 Train loss: 0.5915077924728394 at step: 686 lr 0.0001\n",
      "2024_06_11_06_14_42 Train loss: 0.16656827926635742 at step: 687 lr 0.0001\n",
      "2024_06_11_06_14_43 Train loss: 0.2675066888332367 at step: 688 lr 0.0001\n",
      "2024_06_11_06_14_45 Train loss: 0.1875172108411789 at step: 689 lr 0.0001\n",
      "2024_06_11_06_14_46 Train loss: 0.16920095682144165 at step: 690 lr 0.0001\n",
      "2024_06_11_06_14_47 Train loss: 0.28524988889694214 at step: 691 lr 0.0001\n",
      "2024_06_11_06_14_47 Train loss: 0.32848989963531494 at step: 692 lr 0.0001\n",
      "2024_06_11_06_14_48 Train loss: 0.1706162393093109 at step: 693 lr 0.0001\n",
      "2024_06_11_06_14_50 Train loss: 0.19892995059490204 at step: 694 lr 0.0001\n",
      "2024_06_11_06_14_51 Train loss: 0.3630436062812805 at step: 695 lr 0.0001\n",
      "2024_06_11_06_14_52 Train loss: 0.21148306131362915 at step: 696 lr 0.0001\n",
      "2024_06_11_06_14_54 Train loss: 0.26423054933547974 at step: 697 lr 0.0001\n",
      "2024_06_11_06_14_55 Train loss: 0.16777080297470093 at step: 698 lr 0.0001\n",
      "2024_06_11_06_14_58 Train loss: 0.16106900572776794 at step: 699 lr 0.0001\n",
      "2024_06_11_06_15_00 Train loss: 0.29559314250946045 at step: 700 lr 0.0001\n",
      "2024_06_11_06_15_02 Train loss: 0.14571069180965424 at step: 701 lr 0.0001\n",
      "2024_06_11_06_15_03 Train loss: 0.18934430181980133 at step: 702 lr 0.0001\n",
      "2024_06_11_06_15_06 Train loss: 0.27975642681121826 at step: 703 lr 0.0001\n",
      "2024_06_11_06_15_08 Train loss: 0.25371283292770386 at step: 704 lr 0.0001\n",
      "2024_06_11_06_15_09 Train loss: 0.2306516170501709 at step: 705 lr 0.0001\n",
      "2024_06_11_06_15_10 Train loss: 0.19353927671909332 at step: 706 lr 0.0001\n",
      "2024_06_11_06_15_12 Train loss: 0.22488933801651 at step: 707 lr 0.0001\n",
      "2024_06_11_06_15_13 Train loss: 0.16797930002212524 at step: 708 lr 0.0001\n",
      "2024_06_11_06_15_15 Train loss: 0.16986726224422455 at step: 709 lr 0.0001\n",
      "2024_06_11_06_15_16 Train loss: 0.37279877066612244 at step: 710 lr 0.0001\n",
      "2024_06_11_06_15_17 Train loss: 0.23314842581748962 at step: 711 lr 0.0001\n",
      "2024_06_11_06_15_18 Train loss: 0.1554654985666275 at step: 712 lr 0.0001\n",
      "2024_06_11_06_15_19 Train loss: 0.2500149607658386 at step: 713 lr 0.0001\n",
      "2024_06_11_06_15_20 Train loss: 0.24262335896492004 at step: 714 lr 0.0001\n",
      "2024_06_11_06_15_22 Train loss: 0.3499835133552551 at step: 715 lr 0.0001\n",
      "2024_06_11_06_15_22 Train loss: 0.2912999987602234 at step: 716 lr 0.0001\n",
      "2024_06_11_06_15_23 Train loss: 0.19101175665855408 at step: 717 lr 0.0001\n",
      "2024_06_11_06_15_24 Train loss: 0.18879586458206177 at step: 718 lr 0.0001\n",
      "2024_06_11_06_15_25 Train loss: 0.31096354126930237 at step: 719 lr 0.0001\n",
      "2024_06_11_06_15_26 Train loss: 0.39299458265304565 at step: 720 lr 0.0001\n",
      "2024_06_11_06_15_28 Train loss: 0.3222603499889374 at step: 721 lr 0.0001\n",
      "2024_06_11_06_15_28 Train loss: 0.26618534326553345 at step: 722 lr 0.0001\n",
      "2024_06_11_06_15_31 Train loss: 0.28433680534362793 at step: 723 lr 0.0001\n",
      "2024_06_11_06_15_31 Train loss: 0.17203152179718018 at step: 724 lr 0.0001\n",
      "2024_06_11_06_15_32 Train loss: 0.25246214866638184 at step: 725 lr 0.0001\n",
      "2024_06_11_06_15_34 Train loss: 0.38724416494369507 at step: 726 lr 0.0001\n",
      "2024_06_11_06_15_34 Train loss: 0.1751953661441803 at step: 727 lr 0.0001\n",
      "2024_06_11_06_15_35 Train loss: 0.11408679187297821 at step: 728 lr 0.0001\n",
      "2024_06_11_06_15_37 Train loss: 0.35016363859176636 at step: 729 lr 0.0001\n",
      "2024_06_11_06_15_37 Train loss: 0.576414167881012 at step: 730 lr 0.0001\n",
      "2024_06_11_06_15_39 Train loss: 0.160477876663208 at step: 731 lr 0.0001\n",
      "2024_06_11_06_15_40 Train loss: 0.31300875544548035 at step: 732 lr 0.0001\n",
      "2024_06_11_06_15_41 Train loss: 0.25369763374328613 at step: 733 lr 0.0001\n",
      "2024_06_11_06_15_42 Train loss: 0.5548256039619446 at step: 734 lr 0.0001\n",
      "2024_06_11_06_15_43 Train loss: 0.26917386054992676 at step: 735 lr 0.0001\n",
      "2024_06_11_06_15_44 Train loss: 0.5741310119628906 at step: 736 lr 0.0001\n",
      "2024_06_11_06_15_45 Train loss: 0.15366877615451813 at step: 737 lr 0.0001\n",
      "2024_06_11_06_15_46 Train loss: 0.25149595737457275 at step: 738 lr 0.0001\n",
      "2024_06_11_06_15_47 Train loss: 0.16293400526046753 at step: 739 lr 0.0001\n",
      "2024_06_11_06_15_48 Train loss: 0.5383437871932983 at step: 740 lr 0.0001\n",
      "(Val @ epoch 9) acc: 0.9455782312925171; ap: 0.9885601681808529\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_06_16_09 Train loss: 0.35641494393348694 at step: 741 lr 0.0001\n",
      "2024_06_11_06_16_10 Train loss: 0.17986293137073517 at step: 742 lr 0.0001\n",
      "2024_06_11_06_16_10 Train loss: 0.15859508514404297 at step: 743 lr 0.0001\n",
      "2024_06_11_06_16_11 Train loss: 0.1360490322113037 at step: 744 lr 0.0001\n",
      "2024_06_11_06_16_12 Train loss: 0.2795688509941101 at step: 745 lr 0.0001\n",
      "2024_06_11_06_16_14 Train loss: 0.2461642324924469 at step: 746 lr 0.0001\n",
      "2024_06_11_06_16_16 Train loss: 0.2884877622127533 at step: 747 lr 0.0001\n",
      "2024_06_11_06_16_16 Train loss: 0.18229810893535614 at step: 748 lr 0.0001\n",
      "2024_06_11_06_16_17 Train loss: 0.0972384437918663 at step: 749 lr 0.0001\n",
      "2024_06_11_06_16_18 Train loss: 0.327467143535614 at step: 750 lr 0.0001\n",
      "2024_06_11_06_16_20 Train loss: 0.30489346385002136 at step: 751 lr 0.0001\n",
      "2024_06_11_06_16_22 Train loss: 0.18761545419692993 at step: 752 lr 0.0001\n",
      "2024_06_11_06_16_23 Train loss: 0.37905484437942505 at step: 753 lr 0.0001\n",
      "2024_06_11_06_16_24 Train loss: 0.10906527936458588 at step: 754 lr 0.0001\n",
      "2024_06_11_06_16_26 Train loss: 0.1443365514278412 at step: 755 lr 0.0001\n",
      "2024_06_11_06_16_28 Train loss: 0.10722579061985016 at step: 756 lr 0.0001\n",
      "2024_06_11_06_16_29 Train loss: 0.2611359655857086 at step: 757 lr 0.0001\n",
      "2024_06_11_06_16_31 Train loss: 0.26276838779449463 at step: 758 lr 0.0001\n",
      "2024_06_11_06_16_32 Train loss: 0.10190308839082718 at step: 759 lr 0.0001\n",
      "2024_06_11_06_16_32 Train loss: 0.1508152335882187 at step: 760 lr 0.0001\n",
      "2024_06_11_06_16_35 Train loss: 0.4635758399963379 at step: 761 lr 0.0001\n",
      "2024_06_11_06_16_37 Train loss: 0.27039411664009094 at step: 762 lr 0.0001\n",
      "2024_06_11_06_16_39 Train loss: 0.19306105375289917 at step: 763 lr 0.0001\n",
      "2024_06_11_06_16_41 Train loss: 0.2670958340167999 at step: 764 lr 0.0001\n",
      "2024_06_11_06_16_42 Train loss: 0.24358665943145752 at step: 765 lr 0.0001\n",
      "2024_06_11_06_16_44 Train loss: 0.47849395871162415 at step: 766 lr 0.0001\n",
      "2024_06_11_06_16_45 Train loss: 0.21009591221809387 at step: 767 lr 0.0001\n",
      "2024_06_11_06_16_47 Train loss: 0.19544728100299835 at step: 768 lr 0.0001\n",
      "2024_06_11_06_16_48 Train loss: 0.39301586151123047 at step: 769 lr 0.0001\n",
      "2024_06_11_06_16_50 Train loss: 0.227626770734787 at step: 770 lr 0.0001\n",
      "2024_06_11_06_16_52 Train loss: 0.1635248064994812 at step: 771 lr 0.0001\n",
      "2024_06_11_06_16_53 Train loss: 0.25443053245544434 at step: 772 lr 0.0001\n",
      "2024_06_11_06_16_55 Train loss: 0.19441817700862885 at step: 773 lr 0.0001\n",
      "2024_06_11_06_16_57 Train loss: 0.2662196755409241 at step: 774 lr 0.0001\n",
      "2024_06_11_06_16_59 Train loss: 0.1959163099527359 at step: 775 lr 0.0001\n",
      "2024_06_11_06_17_00 Train loss: 0.18099403381347656 at step: 776 lr 0.0001\n",
      "2024_06_11_06_17_02 Train loss: 0.22642256319522858 at step: 777 lr 0.0001\n",
      "2024_06_11_06_17_04 Train loss: 0.3580605685710907 at step: 778 lr 0.0001\n",
      "2024_06_11_06_17_04 Train loss: 0.11713574081659317 at step: 779 lr 0.0001\n",
      "2024_06_11_06_17_06 Train loss: 0.3698929250240326 at step: 780 lr 0.0001\n",
      "2024_06_11_06_17_08 Train loss: 0.1675490140914917 at step: 781 lr 0.0001\n",
      "2024_06_11_06_17_10 Train loss: 0.3013094663619995 at step: 782 lr 0.0001\n",
      "2024_06_11_06_17_13 Train loss: 0.1481892466545105 at step: 783 lr 0.0001\n",
      "2024_06_11_06_17_16 Train loss: 0.2313062697649002 at step: 784 lr 0.0001\n",
      "2024_06_11_06_17_17 Train loss: 0.4567270278930664 at step: 785 lr 0.0001\n",
      "2024_06_11_06_17_18 Train loss: 0.39987993240356445 at step: 786 lr 0.0001\n",
      "2024_06_11_06_17_20 Train loss: 0.2784598469734192 at step: 787 lr 0.0001\n",
      "2024_06_11_06_17_21 Train loss: 0.25908881425857544 at step: 788 lr 0.0001\n",
      "2024_06_11_06_17_24 Train loss: 0.062101174145936966 at step: 789 lr 0.0001\n",
      "2024_06_11_06_17_25 Train loss: 0.15029865503311157 at step: 790 lr 0.0001\n",
      "2024_06_11_06_17_27 Train loss: 0.38830748200416565 at step: 791 lr 0.0001\n",
      "2024_06_11_06_17_29 Train loss: 0.3151758015155792 at step: 792 lr 0.0001\n",
      "2024_06_11_06_17_31 Train loss: 0.22334158420562744 at step: 793 lr 0.0001\n",
      "2024_06_11_06_17_32 Train loss: 0.2139209657907486 at step: 794 lr 0.0001\n",
      "2024_06_11_06_17_32 Train loss: 0.2092931866645813 at step: 795 lr 0.0001\n",
      "2024_06_11_06_17_34 Train loss: 0.2491343915462494 at step: 796 lr 0.0001\n",
      "2024_06_11_06_17_36 Train loss: 0.1308896243572235 at step: 797 lr 0.0001\n",
      "2024_06_11_06_17_39 Train loss: 0.28709524869918823 at step: 798 lr 0.0001\n",
      "2024_06_11_06_17_40 Train loss: 0.181338369846344 at step: 799 lr 0.0001\n",
      "2024_06_11_06_17_43 Train loss: 0.22957856953144073 at step: 800 lr 0.0001\n",
      "2024_06_11_06_17_44 Train loss: 0.21163532137870789 at step: 801 lr 0.0001\n",
      "2024_06_11_06_17_46 Train loss: 0.14529913663864136 at step: 802 lr 0.0001\n",
      "2024_06_11_06_17_48 Train loss: 0.24554918706417084 at step: 803 lr 0.0001\n",
      "2024_06_11_06_17_50 Train loss: 0.0728236585855484 at step: 804 lr 0.0001\n",
      "2024_06_11_06_17_51 Train loss: 0.21204239130020142 at step: 805 lr 0.0001\n",
      "2024_06_11_06_17_53 Train loss: 0.324187308549881 at step: 806 lr 0.0001\n",
      "2024_06_11_06_17_55 Train loss: 0.24437730014324188 at step: 807 lr 0.0001\n",
      "2024_06_11_06_17_58 Train loss: 0.28098350763320923 at step: 808 lr 0.0001\n",
      "2024_06_11_06_18_00 Train loss: 0.2935008406639099 at step: 809 lr 0.0001\n",
      "2024_06_11_06_18_03 Train loss: 0.3375137150287628 at step: 810 lr 0.0001\n",
      "2024_06_11_06_18_06 Train loss: 0.26908278465270996 at step: 811 lr 0.0001\n",
      "2024_06_11_06_18_07 Train loss: 0.23025180399417877 at step: 812 lr 0.0001\n",
      "2024_06_11_06_18_09 Train loss: 0.3534356355667114 at step: 813 lr 0.0001\n",
      "2024_06_11_06_18_09 Train loss: 0.19551551342010498 at step: 814 lr 0.0001\n",
      "(Val @ epoch 10) acc: 0.9081632653061225; ap: 0.9664339043158054\n",
      "2024_06_11_06_18_30 Train loss: 0.2566267251968384 at step: 815 lr 0.0001\n",
      "2024_06_11_06_18_34 Train loss: 0.36548173427581787 at step: 816 lr 0.0001\n",
      "2024_06_11_06_18_35 Train loss: 0.13488978147506714 at step: 817 lr 0.0001\n",
      "2024_06_11_06_18_36 Train loss: 0.23378171026706696 at step: 818 lr 0.0001\n",
      "2024_06_11_06_18_38 Train loss: 0.13914379477500916 at step: 819 lr 0.0001\n",
      "2024_06_11_06_18_40 Train loss: 0.06558474898338318 at step: 820 lr 0.0001\n",
      "2024_06_11_06_18_40 Train loss: 0.4041283130645752 at step: 821 lr 0.0001\n",
      "2024_06_11_06_18_43 Train loss: 0.2592041492462158 at step: 822 lr 0.0001\n",
      "2024_06_11_06_18_44 Train loss: 0.4428953528404236 at step: 823 lr 0.0001\n",
      "2024_06_11_06_18_46 Train loss: 0.2452724277973175 at step: 824 lr 0.0001\n",
      "2024_06_11_06_18_47 Train loss: 0.09864245355129242 at step: 825 lr 0.0001\n",
      "2024_06_11_06_18_48 Train loss: 0.24585962295532227 at step: 826 lr 0.0001\n",
      "2024_06_11_06_18_51 Train loss: 0.30537986755371094 at step: 827 lr 0.0001\n",
      "2024_06_11_06_18_52 Train loss: 0.15913984179496765 at step: 828 lr 0.0001\n",
      "2024_06_11_06_18_54 Train loss: 0.21765856444835663 at step: 829 lr 0.0001\n",
      "2024_06_11_06_18_55 Train loss: 0.12964242696762085 at step: 830 lr 0.0001\n",
      "2024_06_11_06_18_56 Train loss: 0.271838903427124 at step: 831 lr 0.0001\n",
      "2024_06_11_06_18_58 Train loss: 0.13783405721187592 at step: 832 lr 0.0001\n",
      "2024_06_11_06_18_59 Train loss: 0.2870362102985382 at step: 833 lr 0.0001\n",
      "2024_06_11_06_19_01 Train loss: 0.33820483088493347 at step: 834 lr 0.0001\n",
      "2024_06_11_06_19_02 Train loss: 0.193678617477417 at step: 835 lr 0.0001\n",
      "2024_06_11_06_19_06 Train loss: 0.3930252492427826 at step: 836 lr 0.0001\n",
      "2024_06_11_06_19_07 Train loss: 0.2316092550754547 at step: 837 lr 0.0001\n",
      "2024_06_11_06_19_08 Train loss: 0.30404502153396606 at step: 838 lr 0.0001\n",
      "2024_06_11_06_19_10 Train loss: 0.14662650227546692 at step: 839 lr 0.0001\n",
      "2024_06_11_06_19_11 Train loss: 0.3221205472946167 at step: 840 lr 0.0001\n",
      "2024_06_11_06_19_13 Train loss: 0.25491178035736084 at step: 841 lr 0.0001\n",
      "2024_06_11_06_19_14 Train loss: 0.28951194882392883 at step: 842 lr 0.0001\n",
      "2024_06_11_06_19_16 Train loss: 0.3073030114173889 at step: 843 lr 0.0001\n",
      "2024_06_11_06_19_19 Train loss: 0.4671909213066101 at step: 844 lr 0.0001\n",
      "2024_06_11_06_19_21 Train loss: 0.40780067443847656 at step: 845 lr 0.0001\n",
      "2024_06_11_06_19_22 Train loss: 0.1656120866537094 at step: 846 lr 0.0001\n",
      "2024_06_11_06_19_25 Train loss: 0.2530488967895508 at step: 847 lr 0.0001\n",
      "2024_06_11_06_19_27 Train loss: 0.1322421431541443 at step: 848 lr 0.0001\n",
      "2024_06_11_06_19_30 Train loss: 0.18351513147354126 at step: 849 lr 0.0001\n",
      "2024_06_11_06_19_30 Train loss: 0.22956016659736633 at step: 850 lr 0.0001\n",
      "2024_06_11_06_19_32 Train loss: 0.13127659261226654 at step: 851 lr 0.0001\n",
      "2024_06_11_06_19_36 Train loss: 0.2135624885559082 at step: 852 lr 0.0001\n",
      "2024_06_11_06_19_39 Train loss: 0.2728874087333679 at step: 853 lr 0.0001\n",
      "2024_06_11_06_19_40 Train loss: 0.19750121235847473 at step: 854 lr 0.0001\n",
      "2024_06_11_06_19_43 Train loss: 0.3303902745246887 at step: 855 lr 0.0001\n",
      "2024_06_11_06_19_46 Train loss: 0.2931501567363739 at step: 856 lr 0.0001\n",
      "2024_06_11_06_19_50 Train loss: 0.201160728931427 at step: 857 lr 0.0001\n",
      "2024_06_11_06_19_52 Train loss: 0.15868298709392548 at step: 858 lr 0.0001\n",
      "2024_06_11_06_19_55 Train loss: 0.2299152910709381 at step: 859 lr 0.0001\n",
      "2024_06_11_06_19_57 Train loss: 0.28196483850479126 at step: 860 lr 0.0001\n",
      "2024_06_11_06_20_00 Train loss: 0.27032679319381714 at step: 861 lr 0.0001\n",
      "2024_06_11_06_20_02 Train loss: 0.23322290182113647 at step: 862 lr 0.0001\n",
      "2024_06_11_06_20_04 Train loss: 0.2824985682964325 at step: 863 lr 0.0001\n",
      "2024_06_11_06_20_07 Train loss: 0.2818726897239685 at step: 864 lr 0.0001\n",
      "2024_06_11_06_20_09 Train loss: 0.3278185725212097 at step: 865 lr 0.0001\n",
      "2024_06_11_06_20_11 Train loss: 0.23325318098068237 at step: 866 lr 0.0001\n",
      "2024_06_11_06_20_14 Train loss: 0.259199857711792 at step: 867 lr 0.0001\n",
      "2024_06_11_06_20_16 Train loss: 0.15488222241401672 at step: 868 lr 0.0001\n",
      "2024_06_11_06_20_18 Train loss: 0.2854347825050354 at step: 869 lr 0.0001\n",
      "2024_06_11_06_20_20 Train loss: 0.32523614168167114 at step: 870 lr 0.0001\n",
      "2024_06_11_06_20_23 Train loss: 0.16924315690994263 at step: 871 lr 0.0001\n",
      "2024_06_11_06_20_25 Train loss: 0.1618988811969757 at step: 872 lr 0.0001\n",
      "2024_06_11_06_20_27 Train loss: 0.5039682388305664 at step: 873 lr 0.0001\n",
      "2024_06_11_06_20_29 Train loss: 0.2435160130262375 at step: 874 lr 0.0001\n",
      "2024_06_11_06_20_31 Train loss: 0.23394516110420227 at step: 875 lr 0.0001\n",
      "2024_06_11_06_20_33 Train loss: 0.23582158982753754 at step: 876 lr 0.0001\n",
      "2024_06_11_06_20_34 Train loss: 0.15512540936470032 at step: 877 lr 0.0001\n",
      "2024_06_11_06_20_35 Train loss: 0.13695132732391357 at step: 878 lr 0.0001\n",
      "2024_06_11_06_20_38 Train loss: 0.24888089299201965 at step: 879 lr 0.0001\n",
      "2024_06_11_06_20_39 Train loss: 0.25597280263900757 at step: 880 lr 0.0001\n",
      "2024_06_11_06_20_40 Train loss: 0.2057647705078125 at step: 881 lr 0.0001\n",
      "2024_06_11_06_20_42 Train loss: 0.22200796008110046 at step: 882 lr 0.0001\n",
      "2024_06_11_06_20_42 Train loss: 0.22364960610866547 at step: 883 lr 0.0001\n",
      "2024_06_11_06_20_44 Train loss: 0.32693469524383545 at step: 884 lr 0.0001\n",
      "2024_06_11_06_20_47 Train loss: 0.3238900899887085 at step: 885 lr 0.0001\n",
      "2024_06_11_06_20_49 Train loss: 0.23247480392456055 at step: 886 lr 0.0001\n",
      "2024_06_11_06_20_50 Train loss: 0.16671939194202423 at step: 887 lr 0.0001\n",
      "2024_06_11_06_20_51 Train loss: 0.13949993252754211 at step: 888 lr 0.0001\n",
      "(Val @ epoch 11) acc: 0.9149659863945578; ap: 0.9524041685294935\n",
      "2024_06_11_06_21_10 Train loss: 0.17889469861984253 at step: 889 lr 0.0001\n",
      "2024_06_11_06_21_11 Train loss: 0.25999051332473755 at step: 890 lr 0.0001\n",
      "2024_06_11_06_21_14 Train loss: 0.2193322777748108 at step: 891 lr 0.0001\n",
      "2024_06_11_06_21_16 Train loss: 0.3141990900039673 at step: 892 lr 0.0001\n",
      "2024_06_11_06_21_17 Train loss: 0.19342076778411865 at step: 893 lr 0.0001\n",
      "2024_06_11_06_21_18 Train loss: 0.2906050682067871 at step: 894 lr 0.0001\n",
      "2024_06_11_06_21_19 Train loss: 0.12337783724069595 at step: 895 lr 0.0001\n",
      "2024_06_11_06_21_21 Train loss: 0.11925332993268967 at step: 896 lr 0.0001\n",
      "2024_06_11_06_21_22 Train loss: 0.40400150418281555 at step: 897 lr 0.0001\n",
      "2024_06_11_06_21_24 Train loss: 0.1553059220314026 at step: 898 lr 0.0001\n",
      "2024_06_11_06_21_26 Train loss: 0.14537715911865234 at step: 899 lr 0.0001\n",
      "2024_06_11_06_21_29 Train loss: 0.270153284072876 at step: 900 lr 0.0001\n",
      "2024_06_11_06_21_31 Train loss: 0.19417622685432434 at step: 901 lr 0.0001\n",
      "2024_06_11_06_21_32 Train loss: 0.21278464794158936 at step: 902 lr 0.0001\n",
      "2024_06_11_06_21_34 Train loss: 0.26186561584472656 at step: 903 lr 0.0001\n",
      "2024_06_11_06_21_35 Train loss: 0.42997902631759644 at step: 904 lr 0.0001\n",
      "2024_06_11_06_21_37 Train loss: 0.23896506428718567 at step: 905 lr 0.0001\n",
      "2024_06_11_06_21_40 Train loss: 0.30046796798706055 at step: 906 lr 0.0001\n",
      "2024_06_11_06_21_43 Train loss: 0.3713839054107666 at step: 907 lr 0.0001\n",
      "2024_06_11_06_21_45 Train loss: 0.13328397274017334 at step: 908 lr 0.0001\n",
      "2024_06_11_06_21_47 Train loss: 0.29968881607055664 at step: 909 lr 0.0001\n",
      "2024_06_11_06_21_49 Train loss: 0.22089974582195282 at step: 910 lr 0.0001\n",
      "2024_06_11_06_21_50 Train loss: 0.2825571298599243 at step: 911 lr 0.0001\n",
      "2024_06_11_06_21_53 Train loss: 0.14325401186943054 at step: 912 lr 0.0001\n",
      "2024_06_11_06_21_55 Train loss: 0.15902474522590637 at step: 913 lr 0.0001\n",
      "2024_06_11_06_21_57 Train loss: 0.10705304890871048 at step: 914 lr 0.0001\n",
      "2024_06_11_06_21_58 Train loss: 0.1441231369972229 at step: 915 lr 0.0001\n",
      "2024_06_11_06_22_00 Train loss: 0.2110298126935959 at step: 916 lr 0.0001\n",
      "2024_06_11_06_22_02 Train loss: 0.21529194712638855 at step: 917 lr 0.0001\n",
      "2024_06_11_06_22_04 Train loss: 0.35831117630004883 at step: 918 lr 0.0001\n",
      "2024_06_11_06_22_06 Train loss: 0.16743513941764832 at step: 919 lr 0.0001\n",
      "2024_06_11_06_22_08 Train loss: 0.273624986410141 at step: 920 lr 0.0001\n",
      "2024_06_11_06_22_11 Train loss: 0.29375946521759033 at step: 921 lr 0.0001\n",
      "2024_06_11_06_22_12 Train loss: 0.2450803965330124 at step: 922 lr 0.0001\n",
      "2024_06_11_06_22_14 Train loss: 0.2769300937652588 at step: 923 lr 0.0001\n",
      "2024_06_11_06_22_16 Train loss: 0.28716903924942017 at step: 924 lr 0.0001\n",
      "2024_06_11_06_22_18 Train loss: 0.2101476788520813 at step: 925 lr 0.0001\n",
      "2024_06_11_06_22_19 Train loss: 0.20372432470321655 at step: 926 lr 0.0001\n",
      "2024_06_11_06_22_20 Train loss: 0.22215023636817932 at step: 927 lr 0.0001\n",
      "2024_06_11_06_22_24 Train loss: 0.1590186357498169 at step: 928 lr 0.0001\n",
      "2024_06_11_06_22_25 Train loss: 0.18657994270324707 at step: 929 lr 0.0001\n",
      "2024_06_11_06_22_26 Train loss: 0.1246388629078865 at step: 930 lr 0.0001\n",
      "2024_06_11_06_22_27 Train loss: 0.24759049713611603 at step: 931 lr 0.0001\n",
      "2024_06_11_06_22_31 Train loss: 0.2738482654094696 at step: 932 lr 0.0001\n",
      "2024_06_11_06_22_32 Train loss: 0.20910656452178955 at step: 933 lr 0.0001\n",
      "2024_06_11_06_22_34 Train loss: 0.4946735203266144 at step: 934 lr 0.0001\n",
      "2024_06_11_06_22_36 Train loss: 0.1933649629354477 at step: 935 lr 0.0001\n",
      "2024_06_11_06_22_38 Train loss: 0.0958409458398819 at step: 936 lr 0.0001\n",
      "2024_06_11_06_22_40 Train loss: 0.20521323382854462 at step: 937 lr 0.0001\n",
      "2024_06_11_06_22_42 Train loss: 0.23519796133041382 at step: 938 lr 0.0001\n",
      "2024_06_11_06_22_43 Train loss: 0.18685391545295715 at step: 939 lr 0.0001\n",
      "2024_06_11_06_22_44 Train loss: 0.21739190816879272 at step: 940 lr 0.0001\n",
      "2024_06_11_06_22_45 Train loss: 0.12566642463207245 at step: 941 lr 0.0001\n",
      "2024_06_11_06_22_47 Train loss: 0.23687013983726501 at step: 942 lr 0.0001\n",
      "2024_06_11_06_22_49 Train loss: 0.2889396548271179 at step: 943 lr 0.0001\n",
      "2024_06_11_06_22_51 Train loss: 0.39576584100723267 at step: 944 lr 0.0001\n",
      "2024_06_11_06_22_52 Train loss: 0.2701639235019684 at step: 945 lr 0.0001\n",
      "2024_06_11_06_22_53 Train loss: 0.2839716672897339 at step: 946 lr 0.0001\n",
      "2024_06_11_06_22_55 Train loss: 0.31470125913619995 at step: 947 lr 0.0001\n",
      "2024_06_11_06_22_57 Train loss: 0.30220818519592285 at step: 948 lr 0.0001\n",
      "2024_06_11_06_22_59 Train loss: 0.351396769285202 at step: 949 lr 0.0001\n",
      "2024_06_11_06_23_01 Train loss: 0.1667504757642746 at step: 950 lr 0.0001\n",
      "2024_06_11_06_23_03 Train loss: 0.3051740825176239 at step: 951 lr 0.0001\n",
      "2024_06_11_06_23_05 Train loss: 0.31622326374053955 at step: 952 lr 0.0001\n",
      "2024_06_11_06_23_07 Train loss: 0.2480720579624176 at step: 953 lr 0.0001\n",
      "2024_06_11_06_23_09 Train loss: 0.3414577841758728 at step: 954 lr 0.0001\n",
      "2024_06_11_06_23_11 Train loss: 0.12424972653388977 at step: 955 lr 0.0001\n",
      "2024_06_11_06_23_13 Train loss: 0.32835739850997925 at step: 956 lr 0.0001\n",
      "2024_06_11_06_23_15 Train loss: 0.3532295227050781 at step: 957 lr 0.0001\n",
      "2024_06_11_06_23_17 Train loss: 0.2414332777261734 at step: 958 lr 0.0001\n",
      "2024_06_11_06_23_19 Train loss: 0.1670255959033966 at step: 959 lr 0.0001\n",
      "2024_06_11_06_23_20 Train loss: 0.3206130862236023 at step: 960 lr 0.0001\n",
      "2024_06_11_06_23_22 Train loss: 0.145472913980484 at step: 961 lr 0.0001\n",
      "2024_06_11_06_23_22 Train loss: 0.3420864939689636 at step: 962 lr 0.0001\n",
      "(Val @ epoch 12) acc: 0.95578231292517; ap: 0.9859056733228995\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_best.pth\n",
      "2024_06_11_06_23_41 Train loss: 0.21638987958431244 at step: 963 lr 0.0001\n",
      "2024_06_11_06_23_44 Train loss: 0.1521759033203125 at step: 964 lr 0.0001\n",
      "2024_06_11_06_23_45 Train loss: 0.14877457916736603 at step: 965 lr 0.0001\n",
      "2024_06_11_06_23_47 Train loss: 0.25438520312309265 at step: 966 lr 0.0001\n",
      "2024_06_11_06_23_49 Train loss: 0.25998222827911377 at step: 967 lr 0.0001\n",
      "2024_06_11_06_23_50 Train loss: 0.3865084946155548 at step: 968 lr 0.0001\n",
      "2024_06_11_06_23_53 Train loss: 0.3225463628768921 at step: 969 lr 0.0001\n",
      "2024_06_11_06_23_55 Train loss: 0.1658346951007843 at step: 970 lr 0.0001\n",
      "2024_06_11_06_23_56 Train loss: 0.12358976900577545 at step: 971 lr 0.0001\n",
      "2024_06_11_06_23_59 Train loss: 0.17381694912910461 at step: 972 lr 0.0001\n",
      "2024_06_11_06_24_00 Train loss: 0.24648046493530273 at step: 973 lr 0.0001\n",
      "2024_06_11_06_24_02 Train loss: 0.30394992232322693 at step: 974 lr 0.0001\n",
      "2024_06_11_06_24_03 Train loss: 0.3260391354560852 at step: 975 lr 0.0001\n",
      "2024_06_11_06_24_06 Train loss: 0.19204425811767578 at step: 976 lr 0.0001\n",
      "2024_06_11_06_24_08 Train loss: 0.44630497694015503 at step: 977 lr 0.0001\n",
      "2024_06_11_06_24_09 Train loss: 0.15964587032794952 at step: 978 lr 0.0001\n",
      "2024_06_11_06_24_10 Train loss: 0.2371644377708435 at step: 979 lr 0.0001\n",
      "2024_06_11_06_24_13 Train loss: 0.3006118834018707 at step: 980 lr 0.0001\n",
      "2024_06_11_06_24_16 Train loss: 0.18200665712356567 at step: 981 lr 0.0001\n",
      "2024_06_11_06_24_18 Train loss: 0.23406991362571716 at step: 982 lr 0.0001\n",
      "2024_06_11_06_24_19 Train loss: 0.17369940876960754 at step: 983 lr 0.0001\n",
      "2024_06_11_06_24_20 Train loss: 0.10785256326198578 at step: 984 lr 0.0001\n",
      "2024_06_11_06_24_21 Train loss: 0.094656802713871 at step: 985 lr 0.0001\n",
      "2024_06_11_06_24_23 Train loss: 0.2130405306816101 at step: 986 lr 0.0001\n",
      "2024_06_11_06_24_25 Train loss: 0.1609445959329605 at step: 987 lr 0.0001\n",
      "2024_06_11_06_24_26 Train loss: 0.2851281762123108 at step: 988 lr 0.0001\n",
      "2024_06_11_06_24_29 Train loss: 0.6466720104217529 at step: 989 lr 0.0001\n",
      "2024_06_11_06_24_30 Train loss: 0.3843402862548828 at step: 990 lr 0.0001\n",
      "2024_06_11_06_24_32 Train loss: 0.3939272165298462 at step: 991 lr 0.0001\n",
      "2024_06_11_06_24_34 Train loss: 0.3011358976364136 at step: 992 lr 0.0001\n",
      "2024_06_11_06_24_35 Train loss: 0.20216423273086548 at step: 993 lr 0.0001\n",
      "2024_06_11_06_24_38 Train loss: 0.17356500029563904 at step: 994 lr 0.0001\n",
      "2024_06_11_06_24_39 Train loss: 0.20657217502593994 at step: 995 lr 0.0001\n",
      "2024_06_11_06_24_40 Train loss: 0.1409369707107544 at step: 996 lr 0.0001\n",
      "2024_06_11_06_24_42 Train loss: 0.19037657976150513 at step: 997 lr 0.0001\n",
      "2024_06_11_06_24_44 Train loss: 0.1978297233581543 at step: 998 lr 0.0001\n",
      "2024_06_11_06_24_45 Train loss: 0.10111323744058609 at step: 999 lr 0.0001\n",
      "2024_06_11_06_24_47 Train loss: 0.13892316818237305 at step: 1000 lr 0.0001\n",
      "2024_06_11_06_24_50 Train loss: 0.12258188426494598 at step: 1001 lr 0.0001\n",
      "2024_06_11_06_24_52 Train loss: 0.14419026672840118 at step: 1002 lr 0.0001\n",
      "2024_06_11_06_24_53 Train loss: 0.3619874119758606 at step: 1003 lr 0.0001\n",
      "2024_06_11_06_24_56 Train loss: 0.21726278960704803 at step: 1004 lr 0.0001\n",
      "2024_06_11_06_24_57 Train loss: 0.31103432178497314 at step: 1005 lr 0.0001\n",
      "2024_06_11_06_24_59 Train loss: 0.1986176073551178 at step: 1006 lr 0.0001\n",
      "2024_06_11_06_25_01 Train loss: 0.2776866555213928 at step: 1007 lr 0.0001\n",
      "2024_06_11_06_25_03 Train loss: 0.3199564814567566 at step: 1008 lr 0.0001\n",
      "2024_06_11_06_25_05 Train loss: 0.15562109649181366 at step: 1009 lr 0.0001\n",
      "2024_06_11_06_25_07 Train loss: 0.2755535840988159 at step: 1010 lr 0.0001\n",
      "2024_06_11_06_25_08 Train loss: 0.17489424347877502 at step: 1011 lr 0.0001\n",
      "2024_06_11_06_25_11 Train loss: 0.13974882662296295 at step: 1012 lr 0.0001\n",
      "2024_06_11_06_25_12 Train loss: 0.10615652799606323 at step: 1013 lr 0.0001\n",
      "2024_06_11_06_25_13 Train loss: 0.2563561499118805 at step: 1014 lr 0.0001\n",
      "2024_06_11_06_25_14 Train loss: 0.2518441379070282 at step: 1015 lr 0.0001\n",
      "2024_06_11_06_25_18 Train loss: 0.34761497378349304 at step: 1016 lr 0.0001\n",
      "2024_06_11_06_25_20 Train loss: 0.20580324530601501 at step: 1017 lr 0.0001\n",
      "2024_06_11_06_25_22 Train loss: 0.24238112568855286 at step: 1018 lr 0.0001\n",
      "2024_06_11_06_25_23 Train loss: 0.16157366335391998 at step: 1019 lr 0.0001\n",
      "2024_06_11_06_25_25 Train loss: 0.3626839518547058 at step: 1020 lr 0.0001\n",
      "2024_06_11_06_25_26 Train loss: 0.1058010458946228 at step: 1021 lr 0.0001\n",
      "2024_06_11_06_25_27 Train loss: 0.17083555459976196 at step: 1022 lr 0.0001\n",
      "2024_06_11_06_25_28 Train loss: 0.16648337244987488 at step: 1023 lr 0.0001\n",
      "2024_06_11_06_25_30 Train loss: 0.11488977074623108 at step: 1024 lr 0.0001\n",
      "2024_06_11_06_25_31 Train loss: 0.32120054960250854 at step: 1025 lr 0.0001\n",
      "2024_06_11_06_25_33 Train loss: 0.2200789749622345 at step: 1026 lr 0.0001\n",
      "2024_06_11_06_25_35 Train loss: 0.24863864481449127 at step: 1027 lr 0.0001\n",
      "2024_06_11_06_25_37 Train loss: 0.056170955300331116 at step: 1028 lr 0.0001\n",
      "2024_06_11_06_25_38 Train loss: 0.12367364019155502 at step: 1029 lr 0.0001\n",
      "2024_06_11_06_25_41 Train loss: 0.10497017204761505 at step: 1030 lr 0.0001\n",
      "2024_06_11_06_25_43 Train loss: 0.17084592580795288 at step: 1031 lr 0.0001\n",
      "2024_06_11_06_25_44 Train loss: 0.2049517035484314 at step: 1032 lr 0.0001\n",
      "2024_06_11_06_25_46 Train loss: 0.1192721277475357 at step: 1033 lr 0.0001\n",
      "2024_06_11_06_25_47 Train loss: 0.10464220494031906 at step: 1034 lr 0.0001\n",
      "2024_06_11_06_25_48 Train loss: 0.23491492867469788 at step: 1035 lr 0.0001\n",
      "2024_06_11_06_25_49 Train loss: 0.3808597922325134 at step: 1036 lr 0.0001\n",
      "(Val @ epoch 13) acc: 0.9251700680272109; ap: 0.9662165204294594\n",
      "2024_06_11_06_26_07 Train loss: 0.33253318071365356 at step: 1037 lr 0.0001\n",
      "2024_06_11_06_26_09 Train loss: 0.20093467831611633 at step: 1038 lr 0.0001\n",
      "2024_06_11_06_26_10 Train loss: 0.06536904722452164 at step: 1039 lr 0.0001\n",
      "2024_06_11_06_26_11 Train loss: 0.19935905933380127 at step: 1040 lr 0.0001\n",
      "2024_06_11_06_26_12 Train loss: 0.1824091374874115 at step: 1041 lr 0.0001\n",
      "2024_06_11_06_26_14 Train loss: 0.2523622512817383 at step: 1042 lr 0.0001\n",
      "2024_06_11_06_26_16 Train loss: 0.28757667541503906 at step: 1043 lr 0.0001\n",
      "2024_06_11_06_26_17 Train loss: 0.20086026191711426 at step: 1044 lr 0.0001\n",
      "2024_06_11_06_26_18 Train loss: 0.20668479800224304 at step: 1045 lr 0.0001\n",
      "2024_06_11_06_26_20 Train loss: 0.11239105463027954 at step: 1046 lr 0.0001\n",
      "2024_06_11_06_26_23 Train loss: 0.15290088951587677 at step: 1047 lr 0.0001\n",
      "2024_06_11_06_26_25 Train loss: 0.3669719099998474 at step: 1048 lr 0.0001\n",
      "2024_06_11_06_26_26 Train loss: 0.12954334914684296 at step: 1049 lr 0.0001\n",
      "2024_06_11_06_26_28 Train loss: 0.21524891257286072 at step: 1050 lr 0.0001\n",
      "2024_06_11_06_26_29 Train loss: 0.11756643652915955 at step: 1051 lr 0.0001\n",
      "2024_06_11_06_26_30 Train loss: 0.046093884855508804 at step: 1052 lr 0.0001\n",
      "2024_06_11_06_26_31 Train loss: 0.09768432378768921 at step: 1053 lr 0.0001\n",
      "2024_06_11_06_26_34 Train loss: 0.15324333310127258 at step: 1054 lr 0.0001\n",
      "2024_06_11_06_26_35 Train loss: 0.1720697283744812 at step: 1055 lr 0.0001\n",
      "2024_06_11_06_26_37 Train loss: 0.18825951218605042 at step: 1056 lr 0.0001\n",
      "2024_06_11_06_26_40 Train loss: 0.32524627447128296 at step: 1057 lr 0.0001\n",
      "2024_06_11_06_26_43 Train loss: 0.1193181499838829 at step: 1058 lr 0.0001\n",
      "2024_06_11_06_26_45 Train loss: 0.24954338371753693 at step: 1059 lr 0.0001\n",
      "2024_06_11_06_26_46 Train loss: 0.30806952714920044 at step: 1060 lr 0.0001\n",
      "2024_06_11_06_26_48 Train loss: 0.24878163635730743 at step: 1061 lr 0.0001\n",
      "2024_06_11_06_26_49 Train loss: 0.3174123167991638 at step: 1062 lr 0.0001\n",
      "2024_06_11_06_26_51 Train loss: 0.2701871991157532 at step: 1063 lr 0.0001\n",
      "2024_06_11_06_26_52 Train loss: 0.06624939292669296 at step: 1064 lr 0.0001\n",
      "2024_06_11_06_26_55 Train loss: 0.16678135097026825 at step: 1065 lr 0.0001\n",
      "2024_06_11_06_26_59 Train loss: 0.4618234634399414 at step: 1066 lr 0.0001\n",
      "2024_06_11_06_27_00 Train loss: 0.08470816165208817 at step: 1067 lr 0.0001\n",
      "2024_06_11_06_27_02 Train loss: 0.15154606103897095 at step: 1068 lr 0.0001\n",
      "2024_06_11_06_27_04 Train loss: 0.15859048068523407 at step: 1069 lr 0.0001\n",
      "2024_06_11_06_27_06 Train loss: 0.23977303504943848 at step: 1070 lr 0.0001\n",
      "2024_06_11_06_27_07 Train loss: 0.1850372552871704 at step: 1071 lr 0.0001\n",
      "2024_06_11_06_27_09 Train loss: 0.1925707906484604 at step: 1072 lr 0.0001\n",
      "2024_06_11_06_27_12 Train loss: 0.14219370484352112 at step: 1073 lr 0.0001\n",
      "2024_06_11_06_27_13 Train loss: 0.09104114770889282 at step: 1074 lr 0.0001\n",
      "2024_06_11_06_27_15 Train loss: 0.17618754506111145 at step: 1075 lr 0.0001\n",
      "2024_06_11_06_27_17 Train loss: 0.3413638472557068 at step: 1076 lr 0.0001\n",
      "2024_06_11_06_27_19 Train loss: 0.19680261611938477 at step: 1077 lr 0.0001\n",
      "2024_06_11_06_27_21 Train loss: 0.16964682936668396 at step: 1078 lr 0.0001\n",
      "2024_06_11_06_27_23 Train loss: 0.18088260293006897 at step: 1079 lr 0.0001\n",
      "2024_06_11_06_27_25 Train loss: 0.25250959396362305 at step: 1080 lr 0.0001\n",
      "2024_06_11_06_27_28 Train loss: 0.14049020409584045 at step: 1081 lr 0.0001\n",
      "2024_06_11_06_27_31 Train loss: 0.21178914606571198 at step: 1082 lr 0.0001\n",
      "2024_06_11_06_27_33 Train loss: 0.17442309856414795 at step: 1083 lr 0.0001\n",
      "2024_06_11_06_27_35 Train loss: 0.26659804582595825 at step: 1084 lr 0.0001\n",
      "2024_06_11_06_27_38 Train loss: 0.1687743067741394 at step: 1085 lr 0.0001\n",
      "2024_06_11_06_27_39 Train loss: 0.2061193734407425 at step: 1086 lr 0.0001\n",
      "2024_06_11_06_27_41 Train loss: 0.2962258458137512 at step: 1087 lr 0.0001\n",
      "2024_06_11_06_27_43 Train loss: 0.16270166635513306 at step: 1088 lr 0.0001\n",
      "2024_06_11_06_27_46 Train loss: 0.42045357823371887 at step: 1089 lr 0.0001\n",
      "2024_06_11_06_27_47 Train loss: 0.35096555948257446 at step: 1090 lr 0.0001\n",
      "2024_06_11_06_27_49 Train loss: 0.47061753273010254 at step: 1091 lr 0.0001\n",
      "2024_06_11_06_27_51 Train loss: 0.1057388037443161 at step: 1092 lr 0.0001\n",
      "2024_06_11_06_27_53 Train loss: 0.31786859035491943 at step: 1093 lr 0.0001\n",
      "2024_06_11_06_27_55 Train loss: 0.13200819492340088 at step: 1094 lr 0.0001\n",
      "2024_06_11_06_27_57 Train loss: 0.13196054100990295 at step: 1095 lr 0.0001\n",
      "2024_06_11_06_28_00 Train loss: 0.21377277374267578 at step: 1096 lr 0.0001\n",
      "2024_06_11_06_28_02 Train loss: 0.2743642330169678 at step: 1097 lr 0.0001\n",
      "2024_06_11_06_28_04 Train loss: 0.3610043227672577 at step: 1098 lr 0.0001\n",
      "2024_06_11_06_28_04 Train loss: 0.2504395842552185 at step: 1099 lr 0.0001\n",
      "2024_06_11_06_28_05 Train loss: 0.1148560419678688 at step: 1100 lr 0.0001\n",
      "2024_06_11_06_28_07 Train loss: 0.2737607955932617 at step: 1101 lr 0.0001\n",
      "2024_06_11_06_28_09 Train loss: 0.24544578790664673 at step: 1102 lr 0.0001\n",
      "2024_06_11_06_28_12 Train loss: 0.16387712955474854 at step: 1103 lr 0.0001\n",
      "2024_06_11_06_28_14 Train loss: 0.3222087621688843 at step: 1104 lr 0.0001\n",
      "2024_06_11_06_28_16 Train loss: 0.2133076786994934 at step: 1105 lr 0.0001\n",
      "2024_06_11_06_28_18 Train loss: 0.25284022092819214 at step: 1106 lr 0.0001\n",
      "2024_06_11_06_28_20 Train loss: 0.12789630889892578 at step: 1107 lr 0.0001\n",
      "2024_06_11_06_28_22 Train loss: 0.20838329195976257 at step: 1108 lr 0.0001\n",
      "2024_06_11_06_28_23 Train loss: 0.08071638643741608 at step: 1109 lr 0.0001\n",
      "2024_06_11_06_28_24 Train loss: 0.21791069209575653 at step: 1110 lr 0.0001\n",
      "(Val @ epoch 14) acc: 0.9081632653061225; ap: 0.9505161963312423\n",
      "2024_06_11_06_28_47 Train loss: 0.33153408765792847 at step: 1111 lr 0.0001\n",
      "2024_06_11_06_28_48 Train loss: 0.20628127455711365 at step: 1112 lr 0.0001\n",
      "2024_06_11_06_28_50 Train loss: 0.18135352432727814 at step: 1113 lr 0.0001\n",
      "2024_06_11_06_28_51 Train loss: 0.3337782919406891 at step: 1114 lr 0.0001\n",
      "2024_06_11_06_28_53 Train loss: 0.38391977548599243 at step: 1115 lr 0.0001\n",
      "2024_06_11_06_28_54 Train loss: 0.4836180508136749 at step: 1116 lr 0.0001\n",
      "2024_06_11_06_28_56 Train loss: 0.1415134072303772 at step: 1117 lr 0.0001\n",
      "2024_06_11_06_28_58 Train loss: 0.36724984645843506 at step: 1118 lr 0.0001\n",
      "2024_06_11_06_29_00 Train loss: 0.43740314245224 at step: 1119 lr 0.0001\n",
      "2024_06_11_06_29_02 Train loss: 0.1176135316491127 at step: 1120 lr 0.0001\n",
      "2024_06_11_06_29_03 Train loss: 0.13410025835037231 at step: 1121 lr 0.0001\n",
      "2024_06_11_06_29_07 Train loss: 0.2421923279762268 at step: 1122 lr 0.0001\n",
      "2024_06_11_06_29_08 Train loss: 0.34731313586235046 at step: 1123 lr 0.0001\n",
      "2024_06_11_06_29_10 Train loss: 0.20521515607833862 at step: 1124 lr 0.0001\n",
      "2024_06_11_06_29_11 Train loss: 0.1630573719739914 at step: 1125 lr 0.0001\n",
      "2024_06_11_06_29_12 Train loss: 0.2702082395553589 at step: 1126 lr 0.0001\n",
      "2024_06_11_06_29_15 Train loss: 0.3595060706138611 at step: 1127 lr 0.0001\n",
      "2024_06_11_06_29_17 Train loss: 0.20464178919792175 at step: 1128 lr 0.0001\n",
      "2024_06_11_06_29_18 Train loss: 0.0805426836013794 at step: 1129 lr 0.0001\n",
      "2024_06_11_06_29_20 Train loss: 0.25570768117904663 at step: 1130 lr 0.0001\n",
      "2024_06_11_06_29_22 Train loss: 0.1604190617799759 at step: 1131 lr 0.0001\n",
      "2024_06_11_06_29_24 Train loss: 0.30400311946868896 at step: 1132 lr 0.0001\n",
      "2024_06_11_06_29_25 Train loss: 0.1734335720539093 at step: 1133 lr 0.0001\n",
      "2024_06_11_06_29_26 Train loss: 0.1403869092464447 at step: 1134 lr 0.0001\n",
      "2024_06_11_06_29_28 Train loss: 0.13928470015525818 at step: 1135 lr 0.0001\n",
      "2024_06_11_06_29_30 Train loss: 0.18266545236110687 at step: 1136 lr 0.0001\n",
      "2024_06_11_06_29_32 Train loss: 0.156691312789917 at step: 1137 lr 0.0001\n",
      "2024_06_11_06_29_34 Train loss: 0.17992791533470154 at step: 1138 lr 0.0001\n",
      "2024_06_11_06_29_36 Train loss: 0.28706514835357666 at step: 1139 lr 0.0001\n",
      "2024_06_11_06_29_37 Train loss: 0.4345628619194031 at step: 1140 lr 0.0001\n",
      "2024_06_11_06_29_39 Train loss: 0.21712681651115417 at step: 1141 lr 0.0001\n",
      "2024_06_11_06_29_41 Train loss: 0.24087947607040405 at step: 1142 lr 0.0001\n",
      "2024_06_11_06_29_42 Train loss: 0.18473048508167267 at step: 1143 lr 0.0001\n",
      "2024_06_11_06_29_44 Train loss: 0.2089332938194275 at step: 1144 lr 0.0001\n",
      "2024_06_11_06_29_47 Train loss: 0.20190870761871338 at step: 1145 lr 0.0001\n",
      "2024_06_11_06_29_48 Train loss: 0.22205476462841034 at step: 1146 lr 0.0001\n",
      "2024_06_11_06_29_50 Train loss: 0.24395836889743805 at step: 1147 lr 0.0001\n",
      "2024_06_11_06_29_52 Train loss: 0.09968926757574081 at step: 1148 lr 0.0001\n",
      "2024_06_11_06_29_55 Train loss: 0.16926798224449158 at step: 1149 lr 0.0001\n",
      "2024_06_11_06_29_57 Train loss: 0.17726247012615204 at step: 1150 lr 0.0001\n",
      "2024_06_11_06_29_59 Train loss: 0.2693556249141693 at step: 1151 lr 0.0001\n",
      "2024_06_11_06_30_00 Train loss: 0.090457022190094 at step: 1152 lr 0.0001\n",
      "2024_06_11_06_30_01 Train loss: 0.1485506296157837 at step: 1153 lr 0.0001\n",
      "2024_06_11_06_30_02 Train loss: 0.14496631920337677 at step: 1154 lr 0.0001\n",
      "2024_06_11_06_30_03 Train loss: 0.20236800611019135 at step: 1155 lr 0.0001\n",
      "2024_06_11_06_30_04 Train loss: 0.23371215164661407 at step: 1156 lr 0.0001\n",
      "2024_06_11_06_30_06 Train loss: 0.14788901805877686 at step: 1157 lr 0.0001\n",
      "2024_06_11_06_30_08 Train loss: 0.17424267530441284 at step: 1158 lr 0.0001\n",
      "2024_06_11_06_30_08 Train loss: 0.1946428269147873 at step: 1159 lr 0.0001\n",
      "2024_06_11_06_30_11 Train loss: 0.16875886917114258 at step: 1160 lr 0.0001\n",
      "2024_06_11_06_30_13 Train loss: 0.14661920070648193 at step: 1161 lr 0.0001\n",
      "2024_06_11_06_30_14 Train loss: 0.19258391857147217 at step: 1162 lr 0.0001\n",
      "2024_06_11_06_30_16 Train loss: 0.3274819850921631 at step: 1163 lr 0.0001\n",
      "2024_06_11_06_30_17 Train loss: 0.20879517495632172 at step: 1164 lr 0.0001\n",
      "2024_06_11_06_30_18 Train loss: 0.09142442047595978 at step: 1165 lr 0.0001\n",
      "2024_06_11_06_30_20 Train loss: 0.08233642578125 at step: 1166 lr 0.0001\n",
      "2024_06_11_06_30_21 Train loss: 0.17821384966373444 at step: 1167 lr 0.0001\n",
      "2024_06_11_06_30_22 Train loss: 0.20095336437225342 at step: 1168 lr 0.0001\n",
      "2024_06_11_06_30_23 Train loss: 0.3277277946472168 at step: 1169 lr 0.0001\n",
      "2024_06_11_06_30_26 Train loss: 0.13687269389629364 at step: 1170 lr 0.0001\n",
      "2024_06_11_06_30_29 Train loss: 0.334930419921875 at step: 1171 lr 0.0001\n",
      "2024_06_11_06_30_30 Train loss: 0.25858181715011597 at step: 1172 lr 0.0001\n",
      "2024_06_11_06_30_32 Train loss: 0.30120402574539185 at step: 1173 lr 0.0001\n",
      "2024_06_11_06_30_33 Train loss: 0.583321213722229 at step: 1174 lr 0.0001\n",
      "2024_06_11_06_30_36 Train loss: 0.4005481004714966 at step: 1175 lr 0.0001\n",
      "2024_06_11_06_30_37 Train loss: 0.24779529869556427 at step: 1176 lr 0.0001\n",
      "2024_06_11_06_30_39 Train loss: 0.2699037492275238 at step: 1177 lr 0.0001\n",
      "2024_06_11_06_30_42 Train loss: 0.09375562518835068 at step: 1178 lr 0.0001\n",
      "2024_06_11_06_30_43 Train loss: 0.3244451880455017 at step: 1179 lr 0.0001\n",
      "2024_06_11_06_30_44 Train loss: 0.21159374713897705 at step: 1180 lr 0.0001\n",
      "2024_06_11_06_30_45 Train loss: 0.11438529938459396 at step: 1181 lr 0.0001\n",
      "2024_06_11_06_30_46 Train loss: 0.4159891605377197 at step: 1182 lr 0.0001\n",
      "2024_06_11_06_30_47 Train loss: 0.20591092109680176 at step: 1183 lr 0.0001\n",
      "2024_06_11_06_30_49 Train loss: 0.10173912346363068 at step: 1184 lr 0.0001\n",
      "(Val @ epoch 15) acc: 0.8469387755102041; ap: 0.976898476585833\n",
      "(Test) acc: 0.8843537414965986; ap: 0.9881452089971556\n",
      "Saving model ./checkpoints/experiment_name2024_06_11_05_55_10/model_epoch_last.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Trainer(opt)\n",
    "early_stopping_epochs = 10\n",
    "best_val_acc = 0\n",
    "n_epoch_since_improvement = 0\n",
    "model.train()\n",
    "\n",
    "print(f'cwd: {os.getcwd()}')\n",
    "for epoch in range(opt.niter):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        model.total_steps += 1\n",
    "\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        ts = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "        print(ts, \"Train loss: {} at step: {} lr {}\".format(model.loss, model.total_steps, model.lr))\n",
    "    \n",
    "        if model.total_steps % opt.loss_freq == 0:\n",
    "            train_writer.add_scalar('loss', model.loss, model.total_steps)\n",
    "        \n",
    "    if epoch % opt.delr_freq == 0 and epoch != 0:\n",
    "        ts = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "        print(ts, 'changing lr at the end of epoch %d, iters %d' % (epoch, model.total_steps))\n",
    "        model.adjust_learning_rate()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    acc, ap = validate(model.model, val_loader)[:2]\n",
    "    val_writer.add_scalar('accuracy', acc, model.total_steps)\n",
    "    val_writer.add_scalar('ap', ap, model.total_steps)\n",
    "\n",
    "    print(\"(Val @ epoch {}) acc: {}; ap: {}\".format(epoch, acc, ap))\n",
    "    if acc > best_val_acc:\n",
    "        model.save_networks('best')\n",
    "        best_val_acc = acc\n",
    "    else:\n",
    "        n_epoch_since_improvement += 1\n",
    "        if n_epoch_since_improvement >= early_stopping_epochs:\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "model.eval()\n",
    "acc, ap = validate(model.model, test_loader)[:2]\n",
    "print(\"(Test) acc: {}; ap: {}\".format(acc, ap))\n",
    "model.save_networks('last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ae95f-b9b3-49d9-86dc-54650b091b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fe626-61ac-4682-9c61-fb1cd488114a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmsn",
   "language": "python",
   "name": "bmsn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
